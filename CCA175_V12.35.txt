Exam:CCA175Title:CCA Spark and HadoopDeveloper ExamVendor:ClouderaVersion:V12.35IT Certification Guaranteed, The Easy Way!1NO.1 CORRECT TEXTProblem Scenario 49 : You have been given below code snippet (do a sum of values by key}, withintermediate output.val keysWithValuesList = Array("foo=A", "foo=A", "foo=A", "foo=A", "foo=B", "bar=C","bar=D", "bar=D")val data = sc.parallelize(keysWithValuesl_ist}//Create key value pairsval kv = data.map(_.split("=")).map(v => (v(0), v(l))).cache()val initialCount = 0;val countByKey = kv.aggregateByKey(initialCount)(addToCounts, sumPartitionCounts)Now define two functions (addToCounts, sumPartitionCounts) such, which will produce followingresults.Output 1countByKey.collectres3: Array[(String, Int)] = Array((foo,5), (bar,3))import scala.collection._val initialSet = scala.collection.mutable.HashSet.empty[String]val uniqueByKey = kv.aggregateByKey(initialSet)(addToSet, mergePartitionSets)Now define two functions (addToSet, mergePartitionSets) such, which will produce following results.Output 2:uniqueByKey.collectres4: Array[(String, scala.collection.mutable.HashSet[String])] = Array((foo,Set(B, A}},(bar,Set(C, D}}}Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :val addToCounts = (n: Int, v: String) => n + 1val sumPartitionCounts = (p1: Int, p2: Int} => p1 + p2val addToSet = (s: mutable.HashSet[String], v: String) => s += vval mergePartitionSets = (p1: mutable.HashSet[String], p2: mutable.HashSet[String]) => p1+ += p2NO.2 CORRECT TEXTProblem Scenario 81 : You have been given MySQL DB with following details. You have been givenfollowing product.csv file product.csv productID,productCode,name,quantity,price1001,PEN,Pen Red,5000,1.231002,PEN,Pen Blue,8000,1.251003,PEN,Pen Black,2000,1.251004,PEC,Pencil 2B,10000,0.481005,PEC,Pencil 2H,8000,0.491006,PEC,Pencil HB,0,9999.99Now accomplish following activities.1 . Create a Hive ORC table using SparkSql2 . Load this data in Hive table.3 . Create a Hive parquet table using SparkSQL and load data in it.IT Certification Guaranteed, The Easy Way!2Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create this tile in HDFS under following directory (Without header}/user/cloudera/he/exam/task1/productcsvStep 2 : Now using Spark-shell read the file as RDD// load the data into a new RDDval products = sc.textFile("/user/cloudera/he/exam/task1/product.csv")// Return the first element in this RDDprod u cts.fi rst()Step 3 : Now define the schema using a case classcase class Product(productid: Integer, code: String, name: String, quantity:lnteger, price:Float)Step 4 : create an RDD of Product objectsval prdRDD = products.map(_.split(",")).map(p =>Product(p(0).tolnt,p(1),p(2),p(3}.tolnt,p(4}.toFloat))prdRDD.first()prdRDD.count()Step 5 : Now create data frame val prdDF = prdRDD.toDF()Step 6 : Now store data in hive warehouse directory. (However, table will not be created } importorg.apache.spark.sql.SaveModeprdDF.write.mode(SaveMode.Overwrite).format("orc").saveAsTable("product_orc_table") step 7:Now create table using data stored in warehouse directory. With the help of hive.hiveshow tablesCREATE EXTERNAL TABLE products (productid int,code string,name string .quantity int, price float}STORED AS oreLOCATION 7user/hive/warehouse/product_orc_table';Step 8 : Now create a parquet tableimport org.apache.spark.sql.SaveModeprdDF.write.mode(SaveMode.Overwrite).format("parquet").saveAsTable("product_parquet_ table")Step 9 : Now create table using thisCREATE EXTERNAL TABLE products_parquet (productid int,code string,name string.quantity int, price float}STORED AS parquetLOCATION 7user/hive/warehouse/product_parquet_table';Step 10 : Check data has been loaded or not.Select * from products;Select * from products_parquet;NO.3 CORRECT TEXTProblem Scenario 84 : In Continuation of previous question, please accomplish following activities.1. Select all the products which has product code as null2. Select all the products, whose name starts with Pen and results should be order by Pricedescending order.IT Certification Guaranteed, The Easy Way!33. Select all the products, whose name starts with Pen and results should be order byPrice descending order and quantity ascending order.4. Select top 2 products by priceAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Select all the products which has product code as nullval results = sqlContext.sql(......SELECT' FROM products WHERE code IS NULL......) results. showQ valresults = sqlContext.sql(......SELECT * FROM products WHERE code = NULL ",,M ) results.showQStep 2 : Select all the products , whose name starts with Pen and results should be order by Pricedescending order. val results = sqlContext.sql(......SELECT * FROM productsWHERE name LIKE 'Pen %' ORDER BY price DESC......)results. showQStep 3 : Select all the products , whose name starts with Pen and results should be order by Pricedescending order and quantity ascending order. val results = sqlContext.sql('.....SELECT * FROMproducts WHERE name LIKE 'Pen %' ORDER BY price DESC, quantity......) results. showQStep 4 : Select top 2 products by priceval results = sqlContext.sql(......SELECT' FROM products ORDER BY price descLIMIT2......}results. show()NO.4 CORRECT TEXTProblem Scenario 4: You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.categoriesjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following activities.Import Single table categories (Subset data} to hive managed table , where category_id between 1and 22Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Import Single table (Subset data)sqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -password=cloudera -table=categories -where "\'category_id\' between 1 and 22" --hive- import --m 1Note: Here the ' is the same you find on ~ keyThis command will create a managed table and content will be created in the following directory./user/hive/warehouse/categoriesStep 2 : Check whether table is created or not (In Hive)show tables;select * from categories;IT Certification Guaranteed, The Easy Way!4NO.5 CORRECT TEXTProblem Scenario 13 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following.1. Create a table in retailedb with following definition.CREATE table departments_export (department_id int(11), department_name varchar(45),created_date T1MESTAMP DEFAULT NOWQ);2. Now import the data from following directory into departments_export table,/user/cloudera/departments newAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Login to musql dbmysql --user=retail_dba -password=clouderashow databases; use retail_db; show tables;step 2 : Create a table as given in problem statement.CREATE table departments_export (departmentjd int(11), department_name varchar(45),created_date T1MESTAMP DEFAULT NOW()); show tables;Step 3 : Export data from /user/cloudera/departmentsnew to new table departments_export sqoopexport -connect jdbc:mysql://quickstart:3306/retail_db \-username retaildba \--password cloudera \--table departments_export \-export-dir /user/cloudera/departments_new \-batchStep 4 : Now check the export is correctly done or not. mysql -user*retail_dba - password=clouderashow databases; use retail _db;show tables;select' from departments_export;NO.6 CORRECT TEXTProblem Scenario 96 : Your spark application required extra Java options as below. -XX:+PrintGCDetails-XX:+PrintGCTimeStampsPlease replace the XXX values correctly./bin/spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=talse --conf XXX hadoopexam.jarAnswer:See the explanation for Step by Step Solution and configuration.Explanation:SolutionXXX: Mspark.executoi\extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps"Notes: ./bin/spark-submit \IT Certification Guaranteed, The Easy Way!5--class <maln-class>--master <master-url> \--deploy-mode <deploy-mode> \-conf <key>=<value> \# other options< application-jar> \[application-arguments]Here, conf is used to pass the Spark related contigs which are required for the application to run likeany specific property(executor memory) or if you want to override the default property which is setin Spark-default.conf.NO.7 CORRECT TEXTProblem Scenario 35 : You have been given a file named spark7/EmployeeName.csv(id,name).EmployeeName.csvE01,LokeshE02,BhupeshE03,AmitE04,RatanE05,DineshE06,PavanE07,TejasE08,SheelaE09,KumarE10,Venkat1. Load this file from hdfs and sort it by name and save it back as (id,name) in results directory.However, make sure while saving it should be able to write In a single file.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution:Step 1 : Create file in hdfs (We will do using Hue). However, you can first create in local filesystemand then upload it to hdfs.Step 2 : Load EmployeeName.csv file from hdfs and create PairRDDsval name = sc.textFile("spark7/EmployeeName.csv")val namePairRDD = name.map(x=> (x.split(",")(0),x.split(",")(1)))Step 3 : Now swap namePairRDD RDD.val swapped = namePairRDD.map(item => item.swap)step 4: Now sort the rdd by key.val sortedOutput = swapped.sortByKey()Step 5 : Now swap the result backval swappedBack = sortedOutput.map(item => item.swap}Step 6 : Save the output as a Text file and output must be written in a single file.swappedBack. repartition(1).saveAsTextFile("spark7/result.txt")NO.8 CORRECT TEXTIT Certification Guaranteed, The Easy Way!6Problem Scenario 89 : You have been given below patient data in csv format,patientID,name,dateOfBirth,lastVisitDate1001,Ah Teck,1991-12-31,2012-01-201002,Kumar,2011-10-29,2012-09-201003,Ali,2011-01-30,2012-10-21Accomplish following activities.1 . Find all the patients whose lastVisitDate between current time and '2012-09-15'2 . Find all the patients who born in 20113 . Find all the patients age4 . List patients whose last visited more than 60 days ago5 . Select patients 18 years old or youngerAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1:hdfs dfs -mkdir sparksql3hdfs dfs -put patients.csv sparksql3/Step 2 : Now in spark shell// SQLContext entry point for working with structured dataval sqlContext = neworg.apache.spark.sql.SQLContext(sc)// this is used to implicitly convert an RDD to a DataFrame.import sqlContext.impIicits._// Import Spark SQL data types and Row.import org.apache.spark.sql._// load the data into a new RDDval patients = sc.textFilef'sparksqIS/patients.csv")// Return the first element in this RDDpatients.first()//define the schema using a case classcase class Patient(patientid: Integer, name: String, dateOfBirth:String , lastVisitDate:String)// create an RDD of Product objectsval patRDD = patients.map(_.split(M,M)).map(p => Patient(p(0).tolnt,p(1),p(2),p(3))) patRDD.first()patRDD.count(}// change RDD of Product objects to a DataFrame val patDF = patRDD.toDF()// register the DataFrame as a temp table patDF.registerTempTable("patients"}// Select data from tableval results = sqlContext.sql(......SELECT* FROM patients '.....)// display dataframe in a tabular formatresults.show()//Find all the patients whose lastVisitDate between current time and '2012-09-15' val results =sqlContext.sql(......SELECT * FROM patients WHERETO_DATE(CAST(UNIX_TIMESTAMP(lastVisitDate, 'yyyy-MM-dd') AS TIMESTAMP))BETWEEN '2012-09-15' AND current_timestamp() ORDER BY lastVisitDate......) results.showQ/.Find all the patients who born in 2011IT Certification Guaranteed, The Easy Way!7val results = sqlContext.sql(......SELECT * FROM patients WHEREYEAR(TO_DATE(CAST(UNIXJTlMESTAMP(dateOfBirth, 'yyyy-MM-dd') ASTIMESTAMP))) = 2011 ......)results. show()//Find all the patients ageval results = sqlContext.sql(......SELECT name, dateOfBirth, datediff(current_date(),TO_DATE(CAST(UNIX_TIMESTAMP(dateOfBirth, 'yyyy-MM-dd') AS TlMESTAMP}}}/365AS ageFROM patientsMini >results.show()//List patients whose last visited more than 60 days ago-- List patients whose last visited more than 60 days agoval results = sqlContext.sql(......SELECT name, lastVisitDate FROM patients WHEREdatediff(current_date(), TO_DATE(CAST(UNIX_TIMESTAMP[lastVisitDate, 'yyyy-MM-dd')AS T1MESTAMP))) > 60......);results. showQ;-- Select patients 18 years old or youngerSELECT' FROM patients WHERE TO_DATE(CAST(UNIXJTlMESTAMP(dateOfBirth,'yyyy-MM-dd') AS TIMESTAMP}) > DATE_SUB(current_date(),INTERVAL 18 YEAR); val results =sqlContext.sql(......SELECT' FROM patients WHERETO_DATE(CAST(UNIX_TIMESTAMP(dateOfBirth, 'yyyy-MM--dd') AS TIMESTAMP)) >DATE_SUB(current_date(), T8*365)......);results. showQ;val results = sqlContext.sql(......SELECT DATE_SUB(current_date(), 18*365) FROM patients......);results.show();NO.9 CORRECT TEXTProblem Scenario 40 : You have been given sample data as below in a file called spark15/file1.txt3070811,1963,1096,,"US","CA",,1,3022811,1963,1096,,"US","CA",,1,563033811,1963,1096,,"US","CA",,1,23Below is the code snippet to process this tile.val field= sc.textFile("spark15/f ilel.txt")val mapper = field.map(x=> A)mapper.map(x => x.map(x=> {B})).collectPlease fill in A and B so it can generate below final outputArray(Array(3070811,1963,109G, 0, "US", "CA", 0,1, 0),Array(3022811,1963,1096, 0, "US", "CA", 0,1, 56),Array(3033811,1963,1096, 0, "US", "CA", 0,1, 23))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :A. x.split(","-1)IT Certification Guaranteed, The Easy Way!8B. if (x. isEmpty) 0 else xNO.10 CORRECT TEXTProblem Scenario 46 : You have been given belwo list in scala (name,sex,cost) for each work done.List( ("Deeapak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female",2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) , ("Neeta" , "female", 2000))Now write a Spark program to load this list as an RDD and do the sum of cost for combination ofname and sex (as key)Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create an RDD out of this listval rdd = sc.parallelize(List( ("Deeapak" , "male", 4000}, ("Deepak" , "male", 2000),("Deepika" , "female", 2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000} ,("Neeta" , "female", 2000}}}Step 2 : Convert this RDD in pair RDDval byKey = rdd.map({case (name,sex,cost) => (name,sex)->cost})Step 3 : Now group by Keyval byKeyGrouped = byKey.groupByKeyStep 4 : Nowsum the cost for each groupval result = byKeyGrouped.map{case ((id1,id2),values) => (id1,id2,values.sum)}Step 5 : Save the results result.repartition(1).saveAsTextFile("spark12/result.txt")NO.11 CORRECT TEXTProblem Scenario 79 : You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.orderstable=retail_db.order_itemsjdbc URL = jdbc:mysql://quickstart:3306/retail_dbColumns of products table : (product_id | product categoryid | product_name | product_description| product_prtce | product_image )Please accomplish following activities.1 . Copy "retaildb.products" table to hdfs in a directory p93_products2 . Filter out all the empty prices3 . Sort all the products based on price in both ascending as well as descending order.4 . Sort all the products based on price as well as product_id in descending order.5 . Use the below functions to do data ordering or ranking and fetch top 10 elements top()takeOrdered() sortByKey()Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Import Single table .IT Certification Guaranteed, The Easy Way!9sqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -password=cloudera -table=products -target-dir=p93_products -m 1Note : Please check you dont have space between before or after '=' sign. Sqoop uses theMapReduce framework to copy data from RDBMS to hdfsStep 2 : Step 2 : Read the data from one of the partition, created using above command, hadoop fs -cat p93_products/part-m-00000Step 3 : Load this directory as RDD using Spark and Python (Open pyspark terminal and do following).productsRDD = sc.textFile("p93_products")Step 4 : Filter empty prices, if exists#filter out empty prices linesnonemptyjines = productsRDD.filter(lambda x: len(x.split(",")[4]) > 0)Step 5 : Now sort data based on product_price in order.sortedPriceProducts=nonempty_lines.map(lambdaline:(float(line.split(",")[4]),line.split(",")[2])).sortByKey()for line in sortedPriceProducts.collect(): print(line)Step 6 : Now sort data based on product_price in descending order.sortedPriceProducts=nonempty_lines.map(lambda line:(float(line.split(",")[4]),line.split(",")[2])).sortByKey(False)for line in sortedPriceProducts.collect(): print(line)Step 7 : Get highest price products name.sortedPriceProducts=nonemptyJines.map(lambda line : (float(line.split(",")[4]),line- split(,,,,,)[2]))-sortByKey(False).take(1) print(sortedPriceProducts)Step 8 : Now sort data based on product_price as well as product_id in descending order.#Dont forget to cast string #Tuple as key ((price,id),name)sortedPriceProducts=nonemptyJines.map(lambda line : ((float(lineprint(sortedPriceProducts)Step 9 : Now sort data based on product_price as well as product_id in descending order, using top()function.#Dont forget to cast string#Tuple as key ((price,id),name)sortedPriceProducts=nonemptyJines.map(lambda line: ((float(line.s^^print(sortedPriceProducts)Step 10 : Now sort data based on product_price as ascending and product_id in ascending order,using takeOrdered{) function.#Dont forget to cast string#Tuple as key ((price,id),name) sortedPriceProducts=nonemptyJines.map(lambda line:((float(line.split(","}[4]},int(line.split(","}[0]}},line.split(","}[2]}}.takeOrdered(10, lambda tuple :(tuple[0][0],tuple[0][1]))Step 11 : Now sort data based on product_price as descending and product_id in ascending order,using takeOrdered() function.# Dont forget to cast string# Tuple as key ((price,id},name)# Using minus(-) parameter can help you to make descending ordering , only for numeric value.sortedPrlceProducts=nonemptylines.map(lambda line:((float(line.split(","}[4]},int(line.split(","}[0]}},line.split(","}[2]}}.takeOrdered(10, lambda tuple :(-tuple[0][0],tuple[0][1]}}IT Certification Guaranteed, The Easy Way!10NO.12 CORRECT TEXTProblem Scenario 69 : Write down a Spark Application using Python,In which it read a file "Content.txt" (On hdfs) with following content.And filter out the word which is less than 2 characters and ignore all empty lines.Once doen store the filtered data in a directory called "problem84" (On hdfs)Content.txtHello this is ABCTECH.comThis is ABYTECH.comApache Spark TrainingThis is Spark Learning SessionSpark is faster than MapReduceAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create an application with following code and store it in problem84.py# Import SparkContext and SparkConffrom pyspark import SparkContext, SparkConf# Create configuration object and set App nameconf = SparkConf().setAppName("CCA 175 Problem 84") sc = sparkContext(conf=conf)#load data from hdfscontentRDD = sc.textFile(MContent.txt")#filter out non-empty linesnonemptyjines = contentRDD.filter(lambda x: len(x) > 0)#Split line based on spacewords = nonempty_lines.ffatMap(lambda x: x.split(''}}#filter out all 2 letter wordsfinalRDD = words.filter(lambda x: len(x) > 2)for word in finalRDD.collect():print(word)#Save final data finalRDD.saveAsTextFile("problem84M)step 2 : Submit this applicationspark-submit -master yarn problem84.pyNO.13 CORRECT TEXTProblem Scenario 73 : You have been given data in json format as below.{"first_name":"Ankit", "last_name":"Jain"}{"first_name":"Amir", "last_name":"Khan"}{"first_name":"Rajesh", "last_name":"Khanna"}{"first_name":"Priynka", "last_name":"Chopra"}{"first_name":"Kareena", "last_name":"Kapoor"}{"first_name":"Lokesh", "last_name":"Yadav"}Do the following activity1 . create employee.json file locally.2 . Load this file on hdfsIT Certification Guaranteed, The Easy Way!113 . Register this data as a temp table in Spark using Python.4 . Write select query and print this data.5 . Now save back this selected data in json format.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : create employee.json tile locally.vi employee.json (press insert) past the content.Step 2 : Upload this tile to hdfs, default location hadoop fs -put employee.jsonStep 3 : Write spark script#lmport SQLContextfrom pyspark import SQLContext# Create instance of SQLContext sqIContext = SQLContext(sc)# Load json fileemployee = sqlContext.jsonFile("employee.json")# Register RDD as a temp table employee.registerTempTablef'EmployeeTab"}# Select data from Employee tableemployeelnfo = sqlContext.sql("select * from EmployeeTab"}#lterate data and printfor row in employeelnfo.collect():print(row)Step 4 : Write dataas a Text file employeelnfo.toJSON().saveAsTextFile("employeeJson1")Step 5: Check whether data has been created or not hadoop fs -cat employeeJsonl/part"NO.14 CORRECT TEXTProblem Scenario 68 : You have given a file as below.spark75/f ile1.txtFile contain some text. As given Belowspark75/file1.txtApache Hadoop is an open-source software framework written in Java for distributed storage anddistributed processing of very large data sets on computer clusters built from commodity hardware.All the modules in Hadoop are designed with a fundamental assumption that hardware failures arecommon and should be automatically handled by the frameworkThe core of Apache Hadoop consists of a storage part known as Hadoop Distributed FileSystem (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks anddistributes them across nodes in a cluster. To process data, Hadoop transfers packaged code fornodes to process in parallel based on the data that needs to be processed.his approach takes advantage of data locality nodes manipulating the data they have access to toallow the dataset to be processed faster and more efficiently than it would be in a more conventionalsupercomputer architecture that relies on a parallel file system where computation and data aredistributed via high-speed networkingFor a slightly more complicated task, lets look into splitting up sentences from our documents intoword bigrams. A bigram is pair of successive tokens in some sequence.We will look at building bigrams from the sequences of words in each sentence, and then try to findthe most frequently occuring ones.IT Certification Guaranteed, The Easy Way!12The first problem is that values in each partition of our initial RDD describe lines from the file ratherthan sentences. Sentences may be split over multiple lines. The glom() RDD method is used to createa single entry for each document containing the list of all lines, we can then join the lines up, thenresplit them into sentences using "." as the separator, using flatMap so that every object in our RDDis now a sentence.A bigram is pair of successive tokens in some sequence. Please build bigrams from the sequences ofwords in each sentence, and then try to find the most frequently occuring ones.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create all three tiles in hdfs (We will do using Hue}. However, you can first create in localfilesystem and then upload it to hdfs.Step 2 : The first problem is that values in each partition of our initial RDD describe lines from the filerather than sentences. Sentences may be split over multiple lines.The glom() RDD method is used to create a single entry for each document containing the list of alllines, we can then join the lines up, then resplit them into sentences using "." as the separator, usingflatMap so that every object in our RDD is now a sentence.sentences = sc.textFile("spark75/file1.txt") \ .glom() \map(lambda x: " ".join(x)) \ .flatMap(lambda x: x.spllt("."))Step 3 : Now we have isolated each sentence we can split it into a list of words and extract the wordbigrams from it. Our new RDD contains tuples containing the word bigram (itself a tuple containingthe first and second word) as the first value and the number 1 as the second value. bigrams =sentences.map(lambda x:x.split())\ .flatMap(lambda x: [((x[i],x[i+1]),1)for i in range(0,len(x)-1)])Step 4 : Finally we can apply the same reduceByKey and sort steps that we used in the wordcountexample, to count up the bigrams and sort them in order of descending frequency. In reduceByKeythe key is not an individual word but a bigram.freq_bigrams = bigrams.reduceByKey(lambda x,y:x+y)\map(lambda x:(x[1],x[0])) \sortByKey(False)freq_bigrams.take(10)NO.15 CORRECT TEXTProblem Scenario 21 : You have been given log generating service as below.startjogs (It will generate continuous logs)tailjogs (You can check , what logs are being generated)stopjogs (It will stop the log service)Path where logs are generated using above service : /opt/gen_logs/logs/access.logNow write a flume configuration file named flumel.conf , using that configuration file dumps logs inHDFS file system in a directory called flumel. Flume channel should have following property as well.After every 100 message it should be committed, use non-durable/faster channel and it should beable to hold maximum 1000 eventsSolution :Step 1 : Create flume configuration file, with below configuration for source, sink and channel.#Define source , sink , channel and agent,IT Certification Guaranteed, The Easy Way!13agent1 .sources = source1agent1 .sinks = sink1agent1.channels = channel1# Describe/configure source1agent1 .sources.source1.type = execagent1.sources.source1.command = tail -F /opt/gen logs/logs/access.log## Describe sinklagentl .sinks.sinkl.channel = memory-channelagentl .sinks.sinkl .type = hdfsagentl .sinks.sink1.hdfs.path = flumelagentl .sinks.sinkl.hdfs.fileType = Data Stream# Now we need to define channell property.agent1.channels.channel1.type = memoryagent1.channels.channell.capacity = 1000agent1.channels.channell.transactionCapacity = 100# Bind the source and sink to the channelagent1.sources.source1.channels = channel1agent1.sinks.sink1.channel = channel1Step 2 : Run below command which will use this configuration file and append data in hdfs.Start log service using : startjogsStart flume service:flume-ng agent -conf /home/cloudera/flumeconf -conf-file/home/cloudera/flumeconf/flumel.conf-Dflume.root.logger=DEBUG,INFO,consoleWait for few mins and than stop log service.Stop_logsAnswer:See the explanation for Step by Step Solution and configuration.NO.16 CORRECT TEXTProblem Scenario 88 : You have been given below three filesproduct.csv (Create this file in hdfs)productID,productCode,name,quantity,price,supplierid1001,PEN,Pen Red,5000,1.23,5011002,PEN,Pen Blue,8000,1.25,5011003,PEN,Pen Black,2000,1.25,5011004,PEC,Pencil 2B,10000,0.48,5021005,PEC,Pencil 2H,8000,0.49,5021006,PEC,Pencil HB,0,9999.99,5022001,PEC,Pencil 3B,500,0.52,5012002,PEC,Pencil 4B,200,0.62,5012003,PEC,Pencil 5B,100,0.73,5012004,PEC,Pencil 6B,500,0.47,502supplier.csvsupplierid,name,phone501,ABC Traders,88881111502,XYZ Company,88882222IT Certification Guaranteed, The Easy Way!14503,QQ Corp,88883333products_suppliers.csvproductID,supplierID2001,5012002,5012003,5012004,5022001,503Now accomplish all the queries given in solution.1. It is possible that, same product can be supplied by multiple supplier. Now find each product, itsprice according to each supplier.2. Find all the supllier name, who are supplying 'Pencil 3B'3. Find all the products , which are supplied by ABC Traders.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : It is possible that, same product can be supplied by multiple supplier. Now find each product,its price according to each supplier.val results = sqlContext.sql(......SELECT products.name AS Product Name', price, suppliers.name ASSupplier Name'FROM products_suppliersJOIN products ON products_suppliers.productlD = products.productID JOIN suppliers ONproducts_suppliers.supplierlD = suppliers.supplierlD null t results.show()Step 2 : Find all the supllier name, who are supplying 'Pencil 3B'val results = sqlContext.sql(......SELECT p.name AS 'Product Name", s.name AS "SupplierName'FROM products_suppliers AS psJOIN products AS p ON ps.productID = p.productIDJOIN suppliers AS s ON ps.supplierlD = s.supplierlDWHERE p.name = 'Pencil 3B"",M )results.show()Step 3 : Find all the products , which are supplied by ABC Traders.val results = sqlContext.sql(......SELECT p.name AS 'Product Name", s.name AS "SupplierName'FROM products AS p, products_suppliers AS ps, suppliers AS s WHERE p.productID = ps.productIDAND ps.supplierlD = s.supplierlDAND s.name = 'ABC Traders".....)results. show()NO.17 CORRECT TEXTProblem Scenario 85 : In Continuation of previous question, please accomplish following activities.1. Select all the columns from product table with output header as below. productID AS ID code ASCode name AS Description price AS 'Unit Price'2. Select code and name both separated by ' -' and header name should be ProductDescription'.IT Certification Guaranteed, The Easy Way!153. Select all distinct prices.4 . Select distinct price and name combination.5 . Select all price data sorted by both code and productID combination.6 . count number of products.7 . Count number of products for each code.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Select all the columns from product table with output header as below. productIDAS ID code AS Code name AS Description price AS "Unit Price'val results = sqlContext.sql(......SELECT productID AS ID, code AS Code, name ASDescription, price AS Unit Price' FROM products ORDER BY ID"""results.show()Step 2 : Select code and name both separated by ' -' and header name should be "ProductDescription.val results = sqlContext.sql(......SELECT CONCAT(code,' -', name) AS Product Description, price FROMproducts""" ) results.showQStep 3 : Select all distinct prices.val results = sqlContext.sql(......SELECT DISTINCT price AS Distinct Price" FROM products......)results.show()Step 4 : Select distinct price and name combination.val results = sqlContext.sql(......SELECT DISTINCT price, name FROM products""" ) results. showQStep 5 : Select all price data sorted by both code and productID combination.val results = sqlContext.sql('.....SELECT' FROM products ORDER BY code, productID'.....) results.show()Step 6 : count number of products.val results = sqlContext.sql(......SELECT COUNT(') AS 'Count' FROM products......) results.show()Step 7 : Count number of products for each code.val results = sqlContext.sql(......SELECT code, COUNT('} FROM products GROUP BY code......) results.showQ val results = sqlContext.sql(......SELECT code, COUNT('} AS count FROM productsGROUP BY code ORDER BY count DESC......)results. showQNO.18 CORRECT TEXTProblem Scenario 38 : You have been given an RDD as below,val rdd: RDD[Array[Byte]]Now you have to save this RDD as a SequenceFile. And below is the code snippet.import org.apache.hadoop.io.compress.GzipCodecrdd.map(bytesArray => (A.get(), newB(bytesArray))).saveAsSequenceFile('7output/path",classOt[GzipCodec])What would be the correct replacement for A and B in above snippet.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :A. NullWritableIT Certification Guaranteed, The Easy Way!16B. BytesWritableNO.19 CORRECT TEXTProblem Scenario 59 : You have been given below code snippet.val x = sc.parallelize(1 to 20)val y = sc.parallelize(10 to 30) operationlz.collectWrite a correct code snippet for operationl which will produce desired output, shown below.Array[lnt] = Array(16,12, 20,13,17,14,18,10,19,15,11)Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :val z = x.intersection(y)intersection : Returns the elements in the two RDDs which are the same.NO.20 CORRECT TEXTProblem Scenario 48 : You have been given below Python code snippet, with intermediate output.We want to take a list of records about people and then we want to sum up their ages and countthem.So for this example the type in the RDD will be a Dictionary in the format of {name: NAME, age:AGE,gender:GENDER}.The result type will be a tuple that looks like so (Sum of Ages, Count) people = []people.append({'name':'Amit', 'age':45,'gender':'M'}) people.append({'name':'Ganga','age':43,'gender':'F'})people.append({'name':'John', 'age':28,'gender':'M'})people.append({'name':'Lolita', 'age':33,'gender':'F'})people.append({'name':'Dont Know', 'age':18,'gender':'T'})peopleRdd=sc.parallelize(people) //Create an RDDpeopleRdd.aggregate((0,0), seqOp, combOp) //Output of above line : 167, 5)Now define two operation seqOp and combOp , such thatseqOp : Sum the age of all people as well count them, in each partition. combOp :Combine results from all partitions.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :seqOp = (lambda x,y: (x[0] + y['age'],x[1] + 1))combOp = (lambda x,y: (x[0] + y[0], x[1] + y[1]))NO.21 CORRECT TEXTProblem Scenario 10 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbIT Certification Guaranteed, The Easy Way!17Please accomplish following.1. Create a database named hadoopexam and then create a table named departments in it, withfollowing fields. department_id int, department_name string e.g. location should behdfs://quickstart.cloudera:8020/user/hive/warehouse/hadoopexam.db/departments2. Please import data in existing table created above from retaidb.departments into hive tablehadoopexam.departments.3. Please import data in a non-existing table, means while importing create hive table namedhadoopexam.departments_newAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Go to hive interface and create database.hivecreate database hadoopexam;Step 2. Use the database created in above step and then create table in it. use hadoopexam; showtables;Step 3 : Create table in it.create table departments (department_id int, department_name string);show tables;desc departments;desc formatted departments;Step 4 : Please check following directory must not exist else it will give error, hdfs dfs -Is/user/cloudera/departmentsIf directory already exists, make sure it is not useful and than delete the same.This is the staging directory where Sqoop store the intermediate data before pushing in hive table.hadoop fs -rm -R departmentsStep 5 : Now import data in existing tablesqoop import \-connect jdbc:mysql://quickstart:3306/retail_db \~ username=retail_dba \-password=cloudera \--table departments \-hive-home /user/hive/warehouse \-hive-import \-hive-overwrite \-hive-table hadoopexam.departmentsStep 6 : Check whether data has been loaded or not.hive;use hadoopexam;show tables;select" from departments;desc formatted departments;Step 7 : Import data in non-existing tables in hive and create table while importing.sqoop import \-connect jdbc:mysql://quickstart:3306/retail_db \IT Certification Guaranteed, The Easy Way!18--username=retail_dba \~ password=cloudera \-table departments \-hive-home /user/hive/warehouse \-hive-import \-hive-overwrite \-hive-table hadoopexam.departments_new \-create-hive-tableStep 8 : Check-whether data has been loaded or not.hive;use hadoopexam;show tables;select" from departments_new;desc formatted departments_new;NO.22 CORRECT TEXTProblem Scenario 31 : You have given following two files1 . Content.txt: Contain a huge text file containing space separated words.2 . Remove.txt: Ignore/filter all the words given in this file (Comma Separated).Write a Spark program which reads the Content.txt file and load as an RDD, remove all the wordsfrom a broadcast variables (which is loaded as an RDD of words from Remove.txt).And count the occurrence of the each word and save it as a text file in HDFS.Content.txtHello this is ABCTech.comThis is TechABY.comApache Spark TrainingThis is Spark Learning SessionSpark is faster than MapReduceRemove.txtHello, is, this, theAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create all three files in hdfs in directory called spark2 (We will do using Hue).However, you can first create in local filesystem and then upload it to hdfsStep 2 : Load the Content.txt fileval content = sc.textFile("spark2/Content.txt") //Load the text fileStep 3 : Load the Remove.txt fileval remove = sc.textFile("spark2/Remove.txt") //Load the text fileStep 4 : Create an RDD from remove, However, there is a possibility each word could have trailingspaces, remove those whitespaces as well. We have used two functions here flatMap, map and trim.val removeRDD= remove.flatMap(x=> x.splitf',") ).map(word=>word.trim)//Create an array of wordsStep 5 : Broadcast the variable, which you want to ignoreval bRemove = sc.broadcast(removeRDD.collect().toList) // It should be array of StringsStep 6 : Split the content RDD, so we can have Array of String. val words = content.flatMap(line =>IT Certification Guaranteed, The Easy Way!19line.split(" "))Step 7 : Filter the RDD, so it can have only content which are not present in "BroadcastVariable". val filtered = words.filter{case (word) => !bRemove.value.contains(word)}Step 8 : Create a PairRDD, so we can have (word,1) tuple or PairRDD. val pairRDD = filtered.map(word=> (word,1))Step 9 : Nowdo the word count on PairRDD. val wordCount = pairRDD.reduceByKey(_ + _)Step 10 : Save the output as a Text file.wordCount.saveAsTextFile("spark2/result.txt")NO.23 CORRECT TEXTProblem Scenario 6 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbCompression Codec : org.apache.hadoop.io.compress.SnappyCodecPlease accomplish following.1. Import entire database such that it can be used as a hive tables, it must be created in defaultschema.2. Also make sure each tables file is partitioned in 3 files e.g. part-00000, part-00002, part-000033. Store all the Java files in a directory called java_output to evalute the furtherAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Drop all the tables, which we have created in previous problems. Before implementing thesolution.Login to hive and execute following command.show tables;drop table categories;drop table customers;drop table departments;drop table employee;drop table ordeMtems;drop table orders;drop table products;show tables;Check warehouse directory. hdfs dfs -Is /user/hive/warehouseStep 2 : Now we have cleaned database. Import entire retail db with all the required parameters asproblem statement is asking.sqoop import-all-tables \-m3\-connect jdbc:mysql://quickstart:3306/retail_db \--username=retail_dba \-password=cloudera \IT Certification Guaranteed, The Easy Way!20-hive-import \--hive-overwrite \-create-hive-table \--compress \--compression-codec org.apache.hadoop.io.compress.SnappyCodec \--outdir java_outputStep 3 : Verify the work is accomplished or not.a. Go to hive and check all the tables hiveshow tables;select count(1) from customers;b. Check the-warehouse directory and number of partitions,hdfs dfs -Is /user/hive/warehousehdfs dfs -Is /user/hive/warehouse/categoriesc. Check the output Java directory.Is -Itr java_output/NO.24 CORRECT TEXTProblem Scenario 60 : You have been given below code snippet.val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"}, 3} val b = a.keyBy(_.length) valc = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","woif","bear","bee"), 3) val d =c.keyBy(_.length) operation1Write a correct code snippet for operationl which will produce desired output, shown below.Array[(lnt, (String, String))] = Array((6,(salmon,salmon)), (6,(salmon,rabbit)),(6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)),(6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)),(3,(rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))Answer:See the explanation for Step by Step Solution and configuration.Explanation:solution:b.join(d).collectjoin [Pair]: Performs an inner join using two key-value RDDs. Please note that the keys must begenerally comparable to make this work. keyBy : Constructs two-component tuples(key-value pairs) by applying a function on each data item. The result of the function becomes thedata item becomes the key and the original value of the newly created tuples.NO.25 CORRECT TEXTProblem Scenario 17 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish below assignment.1. Create a table in hive as below, create table departments_hiveOl(department_id int,department_name string, avg_salary int);2. Create another table in mysql using below statement CREATE TABLE IF NOT EXISTSIT Certification Guaranteed, The Easy Way!21departments_hive01(id int, department_name varchar(45), avg_salary int);3. Copy all the data from departments table to departments_hive01 using insert intodepartments_hive01 select a.*, null from departments a;Also insert following records as belowinsert into departments_hive01 values(777, "Not known",1000);insert into departments_hive01 values(8888, null,1000);insert into departments_hive01 values(666, null,1100);4. Now import data from mysql table departments_hive01 to this hive table. Please make sure thatdata should be visible using below hive command. Also, while importing if null value found fordepartment_name column replace it with "" (empty string) and for id column with -999 select * fromdepartments_hive;Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create hive table as below.hiveshow tables;create table departments_hive01(department_id int, department_name string, avgsalary int);Step 2 : Create table in mysql db as well.mysql -user=retail_dba -password=clouderause retail_dbCREATE TABLE IF NOT EXISTS departments_hive01(id int, department_namevarchar(45), avg_salary int);show tables;step 3 : Insert data in mysql table.insert into departments_hive01 select a.*, null from departments a;check data insertsselect' from departments_hive01;Now iserts null records as given in problem. insert into departments_hive01 values(777,"Not known",1000); insert into departments_hive01 values(8888, null,1000); insert intodepartments_hive01 values(666, null,1100);Step 4 : Now import data in hive as per requirement.sqoop import \-connect jdbc:mysql://quickstart:3306/retail_db \~ username=retail_dba \--password=cloudera \-table departments_hive01 \--hive-home /user/hive/warehouse \--hive-import \-hive-overwrite \-hive-table departments_hive0l \--fields-terminated-by '\001' \--null-string M"\--null-non-strlng -999 \-split-by id \IT Certification Guaranteed, The Easy Way!22-m 1Step 5 : Checkthe data in directory.hdfs dfs -Is /user/hive/warehouse/departments_hive01hdfs dfs -cat/user/hive/warehouse/departments_hive01/part"Check data in hive table.Select * from departments_hive01;NO.26 CORRECT TEXTProblem Scenario 14 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following activities.1. Create a csv file named updated_departments.csv with the following contents in local file system.updated_departments.csv2 ,fitness3 ,footwear1 2,fathematics1 3,fcience1 4,engineering1 000,management2. Upload this csv file to hdfs filesystem,3. Now export this data from hdfs to mysql retaildb.departments table. During upload make sureexisting department will just updated and new departments needs to be inserted.4. Now update updated_departments.csv file with below content.2 ,Fitness3 ,Footwear1 2,Fathematics1 3,Science1 4,Engineering1 000,Management2 000,Quality Check5. Now upload this file to hdfs.6. Now export this data from hdfs to mysql retail_db.departments table. During upload make sureexisting department will just updated and no new departments needs to be inserted.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create a csv tile named updateddepartments.csv with give content.Step 2 : Now upload this tile to HDFS.Create a directory called newdata.hdfs dfs -mkdir new_datahdfs dfs -put updated_departments.csv newdata/Step 3 : Check whether tile is uploaded or not. hdfs dfs -Is new_dataIT Certification Guaranteed, The Easy Way!23Step 4 : Export this file to departments table using sqoop.sqoop export --connect jdbc:mysql://quickstart:3306/retail_db \-username retail_dba \--password cloudera \-table departments \--export-dir new_data \-batch \-m 1 \-update-key department_id \-update-mode allowinsertStep 5 : Check whether required data upsert is done or not. mysql --user=retail_dba -password=cloudera show databases; use retail_db;show tables;select" from departments;Step 6 : Update updated_departments.csv file.Step 7 : Override the existing file in hdfs.hdfs dfs -put updated_departments.csv newdata/Step 8 : Now do the Sqoop export as per the requirement.sqoop export --connect jdbc:mysql://quickstart:3306/retail_db \-username retail_dba\--password cloudera \--table departments \--export-dir new_data \--batch \-m 1 \--update-key-department_id \-update-mode updateonlyStep 9 : Check whether required data update is done or not. mysql --user=retail_dba -password=cloudera show databases; use retail db;show tables;select" from departments;NO.27 CORRECT TEXTProblem Scenario 39 : You have been given two filesspark16/file1.txt1,9,52,7,43,8,3spark16/file2.txt1 ,g,h2 ,i,j3 ,k,lLoad these two tiles as Spark RDD and join them to produce the below results(l,((9,5),(g,h)))(2, ((7,4), (i,j))) (3, ((8,3), (k,l)))And write code snippet which will sum the second columns of above joined results (5+4+3).IT Certification Guaranteed, The Easy Way!24Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create tiles in hdfs using Hue.Step 2 : Create pairRDD for both the files.val one = sc.textFile("spark16/file1.txt").map{_.split(",",-1) match {case Array(a, b, c) => (a, ( b, c))} }val two = sc.textFHe(Mspark16/file2.txt").map{_ .split('7\-1) match {case Array(a, b, c) => (a, (b, c))} }Step 3 : Join both the RDD. val joined = one.join(two)Step 4 : Sum second column values.val sum = joined.map {case (_, ((_, num2), (_, _))) => num2.tolnt}.reduce(_ + _)NO.28 CORRECT TEXTProblem Scenario 22 : You have been given below comma separated employee information.name,salary,sex,agealok,100000,male,29jatin,105000,male,32yogesh,134000,male,39ragini,112000,female,35jyotsana,129000,female,39valmiki,123000,male,29Use the netcat service on port 44444, and nc above data line by line. Please do the followingactivities.1. Create a flume conf file using fastest channel, which write data in hive warehouse directory, in atable called flumeemployee (Create hive table as well tor given data).2. Write a hive query to read average salary of all employees.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create hive table forflumeemployee.'CREATE TABLE flumeemployee(name string, salary int, sex string,age int)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ',';IT Certification Guaranteed, The Easy Way!25Step 2 : Create flume configuration file, with below configuration for source, sink and channel andsave it in flume2.conf.#Define source , sink , channel and agent,agent1 .sources = source1agent1 .sinks = sink1agent1.channels = channel1# Describe/configure source1agent1.sources.source1.type = netcatagent1.sources.source1.bind = 127.0.0.1agent1.sources.source1.port = 44444## Describe sink1agent1 .sinks.sink1.channel = memory-channelagent1.sinks.sink1.type = hdfsagent1 .sinks.sink1.hdfs.path = /user/hive/warehouse/flumeemployeehdfs-agent.sinks.hdfs-write.hdfs.writeFormat=Textagent1 .sinks.sink1.hdfs.tileType = Data Stream# Now we need to define channel1 property.agent1.channels.channel1.type = memoryagent1.channels.channel1.capacity = 1000agent1.channels.channel1.transactionCapacity = 100# Bind the source and sink to the channelAgent1 .sources.sourcel.channels = channell agent1 .sinks.sinkl.channel = channel1Step 3 : Run below command which will use this configuration file and append data in hdfs.Start flume service:flume-ng agent -conf /home/cloudera/flumeconf -conf-file/home/cloudera/flumeconf/flume2.conf --name agent1Step 4 : Open another terminal and use the netcat service.nc localhost 44444Step 5 : Enter data line by line.alok,100000.male,29jatin,105000,male,32yogesh,134000,male,39ragini,112000,female,35jyotsana,129000,female,39valmiki,123000,male,29Step 6 : Open hue and check the data is available in hive table or not.step 7 : Stop flume service by pressing ctrl+cStep 8 : Calculate average salary on hive table using below query. You can use either hive commandline tool or hue. select avg(salary) from flumeemployee;NO.29 CORRECT TEXTProblem Scenario 83 : In Continuation of previous question, please accomplish following activities.1. Select all the records with quantity >= 5000 and name starts with 'Pen'2. Select all the records with quantity >= 5000, price is less than 1.24 and name starts with'Pen'3. Select all the records witch does not have quantity >= 5000 and name does not starts with 'Pen'IT Certification Guaranteed, The Easy Way!264. Select all the products which name is 'Pen Red', 'Pen Black'5. Select all the products which has price BETWEEN 1.0 AND 2.0 AND quantityBETWEEN 1000 AND 2000.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Select all the records with quantity >= 5000 and name starts with 'Pen' val results =sqlContext.sql(......SELECT * FROM products WHERE quantity >= 5000 AND name LIKE 'Pen %.......)results.show()Step 2 : Select all the records with quantity >= 5000 , price is less than 1.24 and name starts with 'Pen'val results = sqlContext.sql(......SELECT * FROM products WHERE quantity >= 5000 AND price < 1.24AND name LIKE 'Pen %.......) results. showQStep 3 : Select all the records witch does not have quantity >= 5000 and name does not starts with'Pen' val results = sqlContext.sql('.....SELECT * FROM products WHERE NOT (quantity >= 5000AND name LIKE 'Pen %')......)results. showQStep 4 : Select all the products wchich name is 'Pen Red', 'Pen Black'val results = sqlContext.sql('.....SELECT' FROM products WHERE name IN ('Pen Red','Pen Black')......)results. showQStep 5 : Select all the products which has price BETWEEN 1.0 AND 2.0 AND quantityBETWEEN 1000 AND 2000.val results = sqlContext.sql(......SELECT * FROM products WHERE (price BETWEEN 1.0AND 2.0) AND (quantity BETWEEN 1000 AND 2000)......)results. show()NO.30 CORRECT TEXTProblem Scenario 28 : You need to implement near real time solutions for collecting informationwhen submitted in file with belowDataecho "IBM,100,20160104" >> /tmp/spooldir2/.bb.txtecho "IBM,103,20160105" >> /tmp/spooldir2/.bb.txtmv /tmp/spooldir2/.bb.txt /tmp/spooldir2/bb.txtAfter few minsecho "IBM,100.2,20160104" >> /tmp/spooldir2/.dr.txtecho "IBM,103.1,20160105" >> /tmp/spooldir2/.dr.txtmv /tmp/spooldir2/.dr.txt /tmp/spooldir2/dr.txtYou have been given below directory location (if not available than create it) /tmp/spooldir2.As soon as file committed in this directory that needs to be available in hdfs in/tmp/flume/primary as well as /tmp/flume/secondary location.However, note that/tmp/flume/secondary is optional, if transaction failed which writes in thisdirectory need not to be rollback.Write a flume configuration file named flumeS.conf and use it to load data in hdfs with followingadditional properties .IT Certification Guaranteed, The Easy Way!271 . Spool /tmp/spooldir2 directory2 . File prefix in hdfs sholuld be events3 . File suffix should be .log4 . If file is not committed and in use than it should have _ as prefix.5 . Data should be written as text to hdfsAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create directory mkdir /tmp/spooldir2Step 2 : Create flume configuration file, with below configuration for source, sink and channel andsave it in flume8.conf.agent1 .sources = source1agent1.sinks = sink1a sink1bagent1.channels = channel1a channel1bagent1.sources.source1.channels = channel1a channel1bagent1.sources.source1.selector.type = replicatingagent1.sources.source1.selector.optional = channel1bagent1.sinks.sink1a.channel = channel1aagent1 .sinks.sink1b.channel = channel1bagent1.sources.source1.type = spooldiragent1 .sources.sourcel.spoolDir = /tmp/spooldir2agent1.sinks.sink1a.type = hdfsagent1 .sinks, sink1a.hdfs. path = /tmp/flume/primaryagent1 .sinks.sink1a.hdfs.tilePrefix = eventsagent1 .sinks.sink1a.hdfs.fileSuffix = .logagent1 .sinks.sink1a.hdfs.fileType = Data Streamagent1 .sinks.sink1b.type = hdfsagent1 .sinks.sink1b.hdfs.path = /tmp/flume/secondaryagent1 .sinks.sink1b.hdfs.filePrefix = eventsagent1.sinks.sink1b.hdfs.fileSuffix = .logagent1 .sinks.sink1b.hdfs.fileType = Data Streamagent1.channels.channel1a.type = fileagent1.channels.channel1b.type = memorystep 4 : Run below command which will use this configuration file and append data in hdfs.Start flume service:flume-ng agent -conf /home/cloudera/flumeconf -conf-file/home/cloudera/flumeconf/flume8.conf --name ageStep 5 : Open another terminal and create a file in /tmp/spooldir2/echo "IBM,100,20160104" > /tmp/spooldir2/.bb.txtecho "IBM,103,20160105" > /tmp/spooldir2/.bb.txt mv /tmp/spooldir2/.bb.txt/tmp/spooldir2/bb.txtAfter few minsecho "IBM.100.2,20160104" >/tmp/spooldir2/.dr.txtecho "IBM,103.1,20160105" > /tmp/spooldir2/.dr.txt mv /tmp/spooldir2/.dr.txt/tmp/spooldir2/dr.txtIT Certification Guaranteed, The Easy Way!28NO.31 CORRECT TEXTProblem Scenario 26 : You need to implement near real time solutions for collecting informationwhen submitted in file with below information. You have been given below directory location (if notavailable than create it) /tmp/nrtcontent. Assume your departments upstream service is continuouslycommitting data in this directory as a new file (not stream of data, because it is near real timesolution). As soon as file committed in this directory that needs to be available in hdfs in /tmp/flumelocationDataecho "I am preparing for CCA175 from ABCTECH.com" > /tmp/nrtcontent/.he1.txt mv/tmp/nrtcontent/.he1.txt /tmp/nrtcontent/he1.txtAfter few minsecho "I am preparing for CCA175 from TopTech.com" > /tmp/nrtcontent/.qt1.txt mv/tmp/nrtcontent/.qt1.txt /tmp/nrtcontent/qt1.txtWrite a flume configuration file named flumes.conf and use it to load data in hdfs with followingadditional properties.1 . Spool /tmp/nrtcontent2 . File prefix in hdfs sholuld be events3 . File suffix should be Jog4 . If file is not commited and in use than it should have as prefix.5 . Data should be written as text to hdfsAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create directory mkdir /tmp/nrtcontentStep 2 : Create flume configuration file, with below configuration for source, sink and channel andsave it in flume6.conf.agent1 .sources = source1agent1 .sinks = sink1agent1.channels = channel1agent1 .sources.source1.channels = channel1agent1 .sinks.sink1.channel = channel1agent1 .sources.source1.type = spooldiragent1 .sources.source1.spoolDir = /tmp/nrtcontentagent1 .sinks.sink1 .type = hdfsagent1 .sinks.sink1.hdfs.path = /tmp/flumeagent1.sinks.sink1.hdfs.filePrefix = eventsagent1.sinks.sink1.hdfs.fileSuffix = .logagent1 .sinks.sink1.hdfs.inUsePrefix = _agent1 .sinks.sink1.hdfs.fileType = Data StreamStep 4 : Run below command which will use this configuration file and append data in hdfs.Start flume service:flume-ng agent -conf /home/cloudera/flumeconf -conf-file/home/cloudera/fIumeconf/fIume6.conf --name agent1Step 5 : Open another terminal and create a file in /tmp/nrtcontentecho "I am preparing for CCA175 from ABCTechm.com" > /tmp/nrtcontent/.he1.txt mvIT Certification Guaranteed, The Easy Way!29/tmp/nrtcontent/.he1.txt /tmp/nrtcontent/he1.txtAfter few minsecho "I am preparing for CCA175 from TopTech.com" > /tmp/nrtcontent/.qt1.txt mv/tmp/nrtcontent/.qt1.txt /tmp/nrtcontent/qt1.txtNO.32 CORRECT TEXTProblem Scenario 61 : You have been given below code snippet.val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3) val b = a.keyBy(_.length) valc = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3) val d =c.keyBy(_.length) operationlWrite a correct code snippet for operationl which will produce desired output, shown below.Array[(lnt, (String, Option[String]}}] = Array((6,(salmon,Some(salmon))),(6,(salmon,Some(rabbit))),(6,(salmon,Some(turkey))), (6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))),(6,(salmon,Some(turkey))), (3,(dog,Some(dog))), (3,(dog,Some(cat))),(3,(dog,Some(dog))), (3,(dog,Some(bee))), (3,(rat,Some(dogg)), (3,(rat,Some(cat)j),(3,(rat.Some(gnu))). (3,(rat,Some(bee))), (8,(elephant,None)))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :b.leftOuterJoin(d}.collectleftOuterJoin [Pair]: Performs an left outer join using two key-value RDDs. Please note that the keysmust be generally comparable to make this work keyBy : Constructs two- component tuples (key-value pairs) by applying a function on each data item. Trie result of the function becomes the keyand the original data item becomes the value of the newly created tuples.NO.33 CORRECT TEXTProblem Scenario 90 : You have been given below two filescourse.txtid,course1 ,Hadoop2 ,Spark3 ,HBasefee.txtid,fee2,39003,42004,2900Accomplish the following activities.1. Select all the courses and their fees , whether fee is listed or not.2. Select all the available fees and respective course. If course does not exists still list the fee3. Select all the courses and their fees , whether fee is listed or not. However, ignore records havingfee as null.Answer:See the explanation for Step by Step Solution and configuration.IT Certification Guaranteed, The Easy Way!30Explanation:Solution :Step 1:hdfs dfs -mkdir sparksql4hdfs dfs -put course.txt sparksql4/hdfs dfs -put fee.txt sparksql4/Step 2 : Now in spark shell// load the data into a new RDDval course = sc.textFile("sparksql4/course.txt")val fee = sc.textFile("sparksql4/fee.txt")// Return the first element in this RDDcourse.fi rst()fee.fi rst()//define the schema using a case class case class Course(id: Integer, name: String) case class Fee(id:Integer, fee: Integer)// create an RDD of Product objectsval courseRDD = course.map(_.split(",")).map(c => Course(c(0).tolnt,c(1))) val feeRDD=fee.map(_.split(",")).map(c => Fee(c(0}.tolnt,c(1}.tolnt)) courseRDD.first() courseRDD.count(}feeRDD.first()feeRDD.countQ// change RDD of Product objects to a DataFrame val courseDF = courseRDD.toDF(} val feeDF =feeRDD.toDF{)// register the DataFrame as a temp table courseDF. registerTempTable("course") feeDF.registerTempTablef'fee")// Select data from tableval results = sqlContext.sql(......SELECT' FROM course """ )results. showQval results = sqlContext.sql(......SELECT' FROM fee......)results. showQval results = sqlContext.sql(......SELECT * FROM course LEFT JOIN fee ON course.id = fee.id......)results-showQ val results ="sqlContext.sql(......SELECT * FROM course RIGHT JOIN fee ON course.id =fee.id "MM ) results. showQ val results = sqlContext.sql(......SELECT' FROM course LEFT JOIN fee ONcourse.id = fee.id where fee.id IS NULL" results. show()NO.34 CORRECT TEXTProblem Scenario 58 : You have been given below code snippet.val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2) val b = a.keyBy(_.length)operation1Write a correct code snippet for operationl which will produce desired output, shown below.Array[(lnt, Seq[String])] = Array((4,ArrayBuffer(lion)), (6,ArrayBuffer(spider)),(3,ArrayBuffer(dog, cat)), (5,ArrayBuffer(tiger, eagle}}}Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :b.groupByKey.collectIT Certification Guaranteed, The Easy Way!31groupByKey [Pair]Very similar to groupBy, but instead of supplying a function, the key-component of each pair willautomatically be presented to the partitioner.Listing Variantsdef groupByKeyQ: RDD[(K, lterable[V]}]def groupByKey(numPartittons: Int): RDD[(K, lterable[V] )]def groupByKey(partitioner: Partitioner): RDD[(K, lterable[V])]NO.35 CORRECT TEXTProblem Scenario 91 : You have been given data in json format as below.{"first_name":"Ankit", "last_name":"Jain"}{"first_name":"Amir", "last_name":"Khan"}{"first_name":"Rajesh", "last_name":"Khanna"}{"first_name":"Priynka", "last_name":"Chopra"}{"first_name":"Kareena", "last_name":"Kapoor"}{"first_name":"Lokesh", "last_name":"Yadav"}Do the following activity1 . create employee.json tile locally.2 . Load this tile on hdfs3 . Register this data as a temp table in Spark using Python.4 . Write select query and print this data.5 . Now save back this selected data in json format.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : create employee.json tile locally.vi employee.json (press insert) past the content.Step 2 : Upload this tile to hdfs, default location hadoop fs -put employee.json val employee =sqlContext.read.json("/user/cloudera/employee.json") employee.write.parquet("employee.parquet") val parq_data = sqlContext.read.parquet("employee.parquet")parq_data.registerTempTable("employee")val allemployee = sqlContext.sql("SELeCT' FROM employee")all_employee.show()import org.apache.spark.sql.SaveMode prdDF.write..format("orc").saveAsTable("product ore table"}//Change the codec.sqlContext.setConf("spark.sql.parquet.compression.codec","snappy")employee.write.mode(SaveMode.Overwrite).parquet("employee.parquet")NO.36 CORRECT TEXTProblem Scenario 24 : You have been given below comma separated employee information.Data Set:name,salary,sex,agealok,100000,male,29jatin,105000,male,32yogesh,134000,male,39IT Certification Guaranteed, The Easy Way!32ragini,112000,female,35jyotsana,129000,female,39valmiki,123000,male,29Requirements:Use the netcat service on port 44444, and nc above data line by line. Please do the followingactivities.1. Create a flume conf file using fastest channel, which write data in hive warehouse directory, in atable called flumemaleemployee (Create hive table as well tor given data).2. While importing, make sure only male employee data is stored.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Step 1 : Create hive table for flumeemployee.'CREATE TABLE flumemaleemployee(name string,salary int,sex string,age int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';step 2 : Create flume configuration file, with below configuration for source, sink and channel andsave it in flume4.conf.#Define source , sink, channel and agent.agent1 .sources = source1agent1 .sinks = sink1agent1 .channels = channel1# Describe/configure source1agent1 .sources.source1.type = netcatagent1 .sources.source1.bind = 127.0.0.1agent1.sources.sourcel.port = 44444#Define interceptorsagent1.sources.source1.interceptors=ilagent1 .sources.source1.interceptors.i1.type=regex_filteragent1 .sources.source1.interceptors.i1.regex=femaleagent1 .sources.source1.interceptors.i1.excludeEvents=true## Describe sink1agent1 .sinks, sinkl.channel = memory-channelagent1.sinks.sink1.type = hdfsagent1 .sinks, sinkl. hdfs. path = /user/hive/warehouse/flumemaleemployee hdfs-agent.sinks.hdfs-write.hdfs.writeFormat=Text agentl .sinks.sink1.hdfs.fileType = Data Stream# Now we need to define channel1 property.agent1.channels.channel1.type = memoryagent1.channels.channell.capacity = 1000agent1.channels.channel1.transactionCapacity = 100# Bind the source and sink to the channelIT Certification Guaranteed, The Easy Way!33agent1 .sources.source1.channels = channel1agent1 .sinks.sink1.channel = channel1step 3 : Run below command which will use this configuration file and append data in hdfs.Start flume service:flume-ng agent -conf /home/cloudera/flumeconf -conf-file/home/cloudera/flumeconf/flume4.conf --name agentlStep 4 : Open another terminal and use the netcat service, nc localhost 44444Step 5 : Enter data line by line.alok,100000,male,29jatin,105000,male,32yogesh,134000,male,39ragini,112000,female,35jyotsana,129000,female,39valmiki.123000.male.29Step 6 : Open hue and check the data is available in hive table or not.Step 7 : Stop flume service by pressing ctrl+cStep 8 : Calculate average salary on hive table using below query. You can use either hive commandline tool or hue. select avg(salary) from flumeemployee;NO.37 CORRECT TEXTProblem Scenario 16 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish below assignment.1. Create a table in hive as below.create table departments_hive(department_id int, department_name string);2. Now import data from mysql table departments to this hive table. Please make sure that datashould be visible using below hive command, select" from departments_hiveAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create hive table as said.hiveshow tables;create table departments_hive(department_id int, department_name string);Step 2 : The important here is, when we create a table without delimiter fields. Then default delimiterfor hive is ^A (\001). Hence, while importing data we have to provide proper delimiter.sqoop import \-connect jdbc:mysql://quickstart:3306/retail_db \~ username=retail_dba \-password=cloudera \--table departments \--hive-home /user/hive/warehouse \IT Certification Guaranteed, The Easy Way!34-hive-import \-hive-overwrite \--hive-table departments_hive \--fields-terminated-by '\001'Step 3 : Check-the data in directory.hdfs dfs -Is /user/hive/warehouse/departments_hivehdfs dfs -cat/user/hive/warehouse/departmentshive/part'Check data in hive table.Select * from departments_hive;NO.38 CORRECT TEXTProblem Scenario 32 : You have given three files as below.spark3/sparkdir1/file1.txtspark3/sparkd ir2ffile2.txtspark3/sparkd ir3Zfile3.txtEach file contain some text.spark3/sparkdir1/file1.txtApache Hadoop is an open-source software framework written in Java for distributed storage anddistributed processing of very large data sets on computer clusters built from commodity hardware.All the modules in Hadoop are designed with a fundamental assumption that hardware failures arecommon and should be automatically handled by the framework spark3/sparkdir2/file2.txtThe core of Apache Hadoop consists of a storage part known as Hadoop Distributed FileSystem (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks anddistributes them across nodes in a cluster. To process data, Hadoop transfers packaged code fornodes to process in parallel based on the data that needs to be processed.spark3/sparkdir3/file3.txthis approach takes advantage of data locality nodes manipulating the data they have access to toallow the dataset to be processed faster and more efficiently than it would be in a more conventionalsupercomputer architecture that relies on a parallel file system where computation and data aredistributed via high-speed networkingNow write a Spark code in scala which will load all these three files from hdfs and do the word countby filtering following words. And result should be sorted by word count in reverse order.Filter words ("a","the","an", "as", "a","with","this","these","is","are","in", "for","to","and","The","of")Also please make sure you load all three files as a Single RDD (All three files must be loaded usingsingle API call).You have also been given following codecimport org.apache.hadoop.io.compress.GzipCodecPlease use above codec to compress file, while saving in hdfs.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create all three files in hdfs (We will do using Hue). However, you can first create in localfilesystem and then upload it to hdfs.Step 2 : Load content from all files.IT Certification Guaranteed, The Easy Way!35val content =sc.textFile("spark3/sparkdir1/file1.txt,spark3/sparkdir2/file2.txt,spark3/sparkdir3/file3.txt") //Load the text fileStep 3 : Now create split each line and create RDD of words.val flatContent = content.flatMap(word=>word.split(" "))step 4 : Remove space after each word (trim it)val trimmedContent = f1atContent.map(word=>word.trim)Step 5 : Create an RDD from remove, all the words that needs to be removed.val removeRDD = sc.parallelize(List("a","theM,ManM, "as","a","with","this","these","is","are'\"in'\ "for", "to","and","The","of"))Step 6 : Filter the RDD, so it can have only content which are not present in removeRDD.val filtered = trimmedContent.subtract(removeRDD}Step 7 : Create a PairRDD, so we can have (word,1) tuple or PairRDD. val pairRDD = filtered.map(word=> (word,1))Step 8 : Now do the word count on PairRDD. val wordCount = pairRDD.reduceByKey(_ +_)Step 9 : Now swap PairRDD.val swapped = wordCount.map(item => item.swap)Step 10 : Now revers order the content. val sortedOutput = swapped.sortByKey(false)Step 11 : Save the output as a Text file. sortedOutput.saveAsTextFile("spark3/result")Step 12 : Save compressed output.import org.apache.hadoop.io.compress.GzipCodecsortedOutput.saveAsTextFile("spark3/compressedresult", classOf[GzipCodec])NO.39 CORRECT TEXTProblem Scenario 80 : You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.productsjdbc URL = jdbc:mysql://quickstart:3306/retail_dbColumns of products table : (product_id | product_category_id | product_name |product_description | product_price | product_image )Please accomplish following activities.1. Copy "retaildb.products" table to hdfs in a directory p93_products2. Now sort the products data sorted by product price per category, use productcategoryid colunm togroup by categoryAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Import Single table .sqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -password=cloudera -table=products --target-dir=p93Note : Please check you dont have space between before or after '=' sign. Sqoop uses theMapReduce framework to copy data from RDBMS to hdfsIT Certification Guaranteed, The Easy Way!36Step 2 : Step 2 : Read the data from one of the partition, created using above command, hadoop fs -cat p93_products/part-m-00000Step 3 : Load this directory as RDD using Spark and Python (Open pyspark terminal and do following}.productsRDD = sc.textFile(Mp93_products")Step 4 : Filter empty prices, if exists#filter out empty prices linesNonempty_lines = productsRDD.filter(lambda x: len(x.split(",")[4]) > 0)Step 5 : Create data set like (categroyld, (id,name,price)mappedRDD = nonempty_lines.map(lambda line: (line.split(",")[1], (line.split(",")[0], line.split(",")[2],float(line.split(",")[4])))) tor line in mappedRDD.collect(): print(line)Step 6 : Now groupBy the all records based on categoryld, which a key on mappedRDD it will produceoutput like (categoryld, iterable of all lines for a key/categoryld) groupByCategroyld =mappedRDD.groupByKey() for line in groupByCategroyld.collect():print(line)step 7 : Now sort the data in each category based on price in ascending order.# sorted is a function to sort an iterable, we can also specify, what would be the Key on which wewant to sort in this case we have price on which it needs to be sorted.groupByCategroyld.map(lambda tuple: sorted(tuple[1], key=lambda tupleValue:tupleValue[2])).take(5)Step 8 : Now sort the data in each category based on price in descending order.# sorted is a function to sort an iterable, we can also specify, what would be the Key on which wewant to sort in this case we have price which it needs to be sorted.on groupByCategroyld.map(lambda tuple: sorted(tuple[1], key=lambda tupleValue:tupleValue[2] , reverse=True)).take(5)NO.40 CORRECT TEXTProblem Scenario 37 : ABCTECH.com has done survey on their Exam Products feedback using a webbased form. With the following free text field as input in web ui.Name: StringSubscription Date: StringRating : StringAnd servey data has been saved in a file called spark9/feedback.txtChristopher|Jan 11, 2015|5Kapil|11 Jan, 2015|5Thomas|6/17/2014|5John|22-08-2013|5Mithun|2013|5Jitendra||5Write a spark program using regular expression which will filter all the valid dates and save in twoseparate file (good record and bad record)Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create a file first using Hue in hdfs.Step 2 : Write all valid regular expressions sysntex for checking whether records are having validIT Certification Guaranteed, The Easy Way!37dates or not.val regl =......(\d+)\s(\w{3})(,)\s(\d{4}).......r//11 Jan, 2015val reg2 =......(\d+)(U)(\d+)(U)(\d{4})......s II 6/17/2014val reg3 =......(\d+)(-)(\d+)(-)(\d{4})""".r//22-08-2013val reg4 =......(\w{3})\s(\d+)(,)\s(\d{4})......s II Jan 11, 2015Step 3 : Load the file as an RDD.val feedbackRDD = sc.textFile("spark9/feedback.txt"}Step 4 : As data are pipe separated , hence split the same. val feedbackSplit = feedbackRDD.map(line=> line.split('|'))Step 5 : Now get the valid records as well as , bad records.val validRecords = feedbackSplit.filter(x =>(reg1.pattern.matcher(x(1).trim).matches|reg2.pattern.matcher(x(1).trim).matches|reg3.pattern.matcher(x(1).trim).matches | reg4.pattern.matcher(x(1).trim).matches)) val badRecords =feedbackSplit.filter(x =>!(reg1.pattern.matcher(x(1).trim).matches|reg2.pattern.matcher(x(1).trim).matches|reg3.pattern.matcher(x(1).trim).matches | reg4.pattern.matcher(x(1).trim).matches))Step 6 : Now convert each Array to Stringsval valid =vatidRecords.map(e => (e(0),e(1),e(2)))val bad =badRecords.map(e => (e(0),e(1),e(2)))Step 7 : Save the output as a Text file and output must be written in a single tile,valid.repartition(1).saveAsTextFile("spark9/good.txt")bad.repartition(1).saveAsTextFile("sparkS7bad.txt")NO.41 CORRECT TEXTProblem Scenario 41 : You have been given below code snippet.val aul = sc.parallelize(List (("a" , Array(1,2)), ("b" , Array(1,2)))) val au2 = sc.parallelize(List (("a" ,Array(3)), ("b" , Array(2))))Apply the Spark method, which will generate below output.Array[(String, Array[lnt])] = Array((a,Array(1, 2)), (b,Array(1, 2)), (a(Array(3)), (b,Array(2)))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution:au1.union(au2)NO.42 CORRECT TEXTProblem Scenario GG : You have been given below code snippet.val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2) val b = a.keyBy(_.length) valc = sc.parallelize(List("ant", "falcon", "squid"), 2) val d = c.keyBy(.length)operation 1Write a correct code snippet for operationl which will produce desired output, shown below.Array[(lnt, String)] = Array((4,lion))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :IT Certification Guaranteed, The Easy Way!38b.subtractByKey(d).collectsubtractByKey [Pair] : Very similar to subtract, but instead of supplying a function, the key-component of each pair will be automatically used as criterion for removing items from the first RDD.NO.43 CORRECT TEXTProblem Scenario 67 : You have been given below code snippet.lines = sc.parallelize(['lts fun to have fun,','but you have to know how.'])M = lines.map( lambda x: x.replace(',7 ').replace('.',' 'J.replaceC-V ').lower()) r2 = r1.flatMap(lambda x:x.split()) r3 = r2.map(lambda x: (x, 1)) operation1r5 = r4.map(lambda x:(x[1],x[0]))r6 = r5.sortByKey(ascending=False)r6.take(20)Write a correct code snippet for operationl which will produce desired output, shown below.[(2, 'fun'), (2, 'to'), (2, 'have'), (1, its'), (1, 'know1), (1, 'how1), (1, 'you'), (1, 'but')]Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :r4 = r3.reduceByKey(lambda x,y:x+y)NO.44 CORRECT TEXTProblem Scenario 11 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following.1. Import departments table in a directory called departments.2. Once import is done, please insert following 5 records in departments mysql table.Insert into departments(10, physics);Insert into departments(11, Chemistry);Insert into departments(12, Maths);Insert into departments(13, Science);Insert into departments(14, Engineering);3. Now import only new inserted records and append to existring directory . which has been createdin first step.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Clean already imported data. (In real exam, please make sure you dont delete data generatedfrom previous exercise).hadoop fs -rm -R departmentsStep 2 : Import data in departments directory.sqoop import \--connect jdbc:mysql://quickstart:3306/retail_db \IT Certification Guaranteed, The Easy Way!39--username=retail_dba \-password=cloudera \-table departments \"target-dir/user/cloudera/departmentsStep 3 : Insert the five records in departments table.mysql -user=retail_dba --password=cloudera retail_dbInsert into departments values(10, "physics"); Insert into departments values(11,"Chemistry"); Insert into departments values(12, "Maths"); Insert into departments values(13,"Science"); Insert into departments values(14, "Engineering"); commit; select' from departments;Step 4 : Get the maximum value of departments from last import, hdfs dfs -cat/user/cloudera/departments/part* that should be 7Step 5 : Do the incremental import based on last import and append the results.sqoop import \--connect "jdbc:mysql://quickstart.cloudera:330G/retail_db" \~ username=retail_dba \-password=cloudera \-table departments \--target-dir /user/cloudera/departments \-append \-check-column "department_id" \-incremental append \-last-value 7Step 6 : Now check the result.hdfs dfs -cat /user/cloudera/departments/part"NO.45 CORRECT TEXTProblem Scenario 70 : Write down a Spark Application using Python, In which it read a file"Content.txt" (On hdfs) with following content. Do the word count and save the results in a directorycalled "problem85" (On hdfs)Content.txtHello this is ABCTECH.comThis is XYZTECH.comApache Spark TrainingThis is Spark Learning SessionSpark is faster than MapReduceAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create an application with following code and store it in problem84.py# Import SparkContext and SparkConffrom pyspark import SparkContext, SparkConf# Create configuration object and set App nameconf = SparkConf().setAppName("CCA 175 Problem 85") sc = sparkContext(conf=conf)#load data from hdfscontentRDD = sc.textFile(MContent.txt")IT Certification Guaranteed, The Easy Way!40#filter out non-empty linesnonemptyjines = contentRDD.filter(lambda x: len(x) > 0)#Split line based on spacewords = nonempty_lines.ffatMap(lambda x: x.split(''}}#Do the word countwordcounts = words.map(lambda x: (x, 1)) \reduceByKey(lambda x, y: x+y) \map(lambda x: (x[1], x[0]}}.sortByKey(False}for word in wordcounts.collect(): print(word)#Save final data " wordcounts.saveAsTextFile("problem85")step 2 : Submit this applicationspark-submit -master yarn problem85.pyNO.46 CORRECT TEXTProblem Scenario 92 : You have been given a spark scala application, which is bundled in jar namedhadoopexam.jar.Your application class name is com.hadoopexam.MyTaskYou want that while submitting your application should launch a driver on one of the cluster node.Please complete the following command to submit the application.spark-submit XXX -master yarn \YYY SSPARK HOME/lib/hadoopexam.jar 10Answer:See the explanation for Step by Step Solution and configuration.Explanation:SolutionXXX: -class com.hadoopexam.MyTaskYYY : --deploy-mode clusterNO.47 CORRECT TEXTProblem Scenario 50 : You have been given below code snippet (calculating an average score}, withintermediate output.type ScoreCollector = (Int, Double)type PersonScores = (String, (Int, Double))val initialScores = Array(("Fred", 88.0), ("Fred", 95.0), ("Fred", 91.0), ("Wilma", 93.0),("Wilma", 95.0), ("Wilma", 98.0))val wilmaAndFredScores = sc.parallelize(initialScores).cache()val scores = wilmaAndFredScores.combineByKey(createScoreCombiner, scoreCombiner,scoreMerger) val averagingFunction = (personScore: PersonScores) => { val (name, (numberScores,totalScore)) = personScore (name, totalScore / numberScores)}val averageScores = scores.collectAsMap(}.map(averagingFunction)Expected output: averageScores: scala.collection.Map[String,Double] = Map(Fred ->91.33333333333333, Wilma -> 95.33333333333333)Define all three required function , which are input for combineByKey method, e.g.(createScoreCombiner, scoreCombiner, scoreMerger). And help us producing required results.Answer:IT Certification Guaranteed, The Easy Way!41See the explanation for Step by Step Solution and configuration.Explanation:Solution :val createScoreCombiner = (score: Double) => (1, score)val scoreCombiner = (collector: ScoreCollector, score: Double) => {val (numberScores. totalScore) = collector (numberScores + 1, totalScore + score)}val scoreMerger= (collector-!: ScoreCollector, collector2: ScoreCollector) => { val(numScoresl. totalScorel) = collector! val (numScores2, tota!Score2) = collector(numScoresl + numScores2, totalScorel + totalScore2)}NO.48 CORRECT TEXTProblem Scenario 20 : You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.categoriesjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following activities.1. Write a Sqoop Job which will import "retaildb.categories" table to hdfs, in a directory name"categories_targetJob".Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Connecting to existing MySQL Database mysql -user=retail_dba -- password=clouderaretail_dbStep 2 : Show all the available tables show tables;Step 3 : Below is the command to create Sqoop Job (Please note that - import space is mandatory)sqoop job -create sqoopjob \ -- import \-connect "jdbc:mysql://quickstart:3306/retail_db" \-username=retail_dba \-password=cloudera \-table categories \-target-dir categories_targetJob \-fields-terminated-by '|' \-lines-terminated-by '\n'Step 4 : List all the Sqoop Jobs sqoop job --listStep 5 : Show details of the Sqoop Job sqoop job --show sqoopjobStep 6 : Execute the sqoopjob sqoopjob --exec sqoopjobStep 7 : Check the output of import jobhdfs dfs -Is categories_target_jobhdfs dfs -cat categories_target_job/part*NO.49 CORRECT TEXTIT Certification Guaranteed, The Easy Way!42Problem Scenario 94 : You have to run your Spark application on yarn with each executor20GB and number of executors should be 50. Please replace XXX, YYY, ZZZ exportHADOOP_CONF_DIR=XXX./bin/spark-submit \-class com.hadoopexam.MyTask \xxx\-deploy-mode cluster \ # can be client for client modeYYY\2 22 \/path/to/hadoopexam.jar \1 000Answer:See the explanation for Step by Step Solution and configuration.Explanation:SolutionXXX: -master yarnYYY : -executor-memory 20GZZZ: -num-executors 50NO.50 CORRECT TEXTProblem Scenario 53 : You have been given below code snippet.val a = sc.parallelize(1 to 10, 3)operation1b.collectOutput 1Array[lnt] = Array(2, 4, 6, 8,10)operation2Output 2Array[lnt] = Array(1,2, 3)Write a correct code snippet for operation1 and operation2 which will produce desired output,shown above.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :valb = a.filter(_%2==0)a.filter(_ < 4).collectfilterEvaluates a boolean function for each data item of the RDD and puts the items for which the functionreturned true into the resulting RDD.When you provide a filter function, it must be able to handle all data items contained in theRDD. Scala provides so-called partial functions to deal with mixed data types (Tip: Partial functions todeal are very useful if you have some data which may be bad and you do not want to handle but forthe good data (matching data) you want to apply some Kind of map function. The following article isgood. It teaches you about partial functions in a very nice way and explains why case has to be usedfor partial functions:article)IT Certification Guaranteed, The Easy Way!43Examples for mixed data without partial functionsval b = sc.parallelize(1 to 8)b.filter(_ < 4)xollectres15: Arrayjlnt] = Array(1, 2, 3)val a = sc.parallelize(List("cat'\ "horse", 4.0, 3.5, 2, "dog"))a.filter(_<4).collecterror: value < is not a member of AnyNO.51 CORRECT TEXTProblem Scenario 5 : You have been given following mysql database details.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following activities.1. List all the tables using sqoop command from retail_db2. Write simple sqoop eval command to check whether you have permission to read database tablesor not.3 . Import all the tables as avro files in /user/hive/warehouse/retail cca174.db4 . Import departments table as a text file in /user/cloudera/departments.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution:Step 1 : List tables using sqoopsqoop list-tables --connect jdbc:mysql://quickstart:330G/retail_db --username retail dba - passwordclouderaStep 2 : Eval command, just run a count query on one of the table.sqoop eval \--connect jdbc:mysql://quickstart:3306/retail_db \-username retail_dba \-password cloudera \--query "select count(1) from ordeMtems"Step 3 : Import all the tables as avro file.sqoop import-all-tables \-connect jdbc:mysql://quickstart:3306/retail_db \-username=retail_dba \-password=cloudera \-as-avrodatafile \-warehouse-dir=/user/hive/warehouse/retail stage.db \-mlStep 4 : Import departments table as a text file in /user/cloudera/departments sqoop import \-connect jdbc:mysql://quickstart:3306/retail_db \-username=retail_dba \-password=cloudera \-table departments \IT Certification Guaranteed, The Easy Way!44-as-textfile \-target-dir=/user/cloudera/departmentsStep 5 : Verify the imported data.hdfs dfs -Is /user/cloudera/departmentshdfs dfs -Is /user/hive/warehouse/retailstage.dbhdfs dfs -Is /user/hive/warehouse/retail_stage.db/productsNO.52 CORRECT TEXTProblem Scenario 29 : Please accomplish the following exercises using HDFS command line options.1. Create a directory in hdfs named hdfs_commands.2. Create a file in hdfs named data.txt in hdfs_commands.3. Now copy this data.txt file on local filesystem, however while copying file please make sure fileproperties are not changed e.g. file permissions.4. Now create a file in local directory named data_local.txt and move this file to hdfs inhdfs_commands directory.5. Create a file data_hdfs.txt in hdfs_commands directory and copy it to local file system.6. Create a file in local filesystem named file1.txt and put it to hdfsAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create directoryhdfs dfs -mkdir hdfs_commandsStep 2 : Create a file in hdfs named data.txt in hdfs_commands. hdfs dfs -touchzhdfs_commands/data.txtStep 3 : Now copy this data.txt file on local filesystem, however while copying file please make surefile properties are not changed e.g. file permissions.hdfs dfs -copyToLocal -p hdfs_commands/data.txt/home/cloudera/Desktop/HadoopExamStep 4 : Now create a file in local directory named data_local.txt and move this file to hdfs inhdfs_commands directory.touch data_local.txthdfs dfs -moveFromLocal /home/cloudera/Desktop/HadoopExam/dataJocal.txt hdfs_commands/Step 5 : Create a file data_hdfs.txt in hdfs_commands directory and copy it to local file system.hdfs dfs -touchz hdfscommands/data hdfs.txthdfs dfs -getfrdfs_commands/data_hdfs.txt /home/cloudera/Desktop/HadoopExam/Step 6 : Create a file in local filesystem named filel .txt and put it to hdfs touch filel.txt hdfs dfs -put/home/cloudera/Desktop/HadoopExam/file1.txt hdfs_commands/NO.53 CORRECT TEXTProblem Scenario 62 : You have been given below code snippet.val a = sc.parallelize(List("dogM, "tiger", "lion", "cat", "panther", "eagle"), 2) val b = a.map(x =>(x.length, x)) operation1Write a correct code snippet for operationl which will produce desired output, shown below.Array[(lnt, String)] = Array((3,xdogx), (5,xtigerx), (4,xlionx), (3,xcatx), (7,xpantherx),(5,xeaglex))Answer:IT Certification Guaranteed, The Easy Way!45See the explanation for Step by Step Solution and configuration.Explanation:Solution :b.mapValuesf'x" + _ + "x").collectmapValues [Pair] : Takes the values of a RDD that consists of two-component tuples, and applies theprovided function to transform each value. Tlien,.it.forms newtwo-componend tuples using the keyand the transformed value and stores them in a new RDD.NO.54 CORRECT TEXTProblem Scenario 45 : You have been given 2 files , with the content as given Below(spark12/technology.txt)(spark12/salary.txt)(spark12/technology.txt)first,last,technologyAmit,Jain,javaLokesh,kumar,unixMithun,kale,sparkRajni,vekat,hadoopRahul,Yadav,scala(spark12/salary.txt)first,last,salaryAmit,Jain,100000Lokesh,kumar,95000Mithun,kale,150000Rajni,vekat,154000Rahul,Yadav,120000Write a Spark program, which will join the data based on first and last name and save the joinedresults in following format, first Last.technology.salaryAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create 2 files first using Hue in hdfs.Step 2 : Load all file as an RDDval technology = sc.textFile(Msparkl2/technology.txt").map(e => e.splitf',")) val salary =sc.textFile("spark12/salary.txt").map(e => e.split("."))Step 3 : Now create Key.value pair of data and join them.val joined = technology.map(e=>((e(0),e(1)),e(2))).join(salary.map(e=>((e(0),e(1)),e(2))))Step 4 : Save the results in a text file as below.joined.repartition(1).saveAsTextFile("spark12/multiColumn Joined.txt")NO.55 CORRECT TEXTProblem Scenario 63 : You have been given below code snippet.val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2) val b = a.map(x =>(x.length, x)) operation1Write a correct code snippet for operationl which will produce desired output, shown below.IT Certification Guaranteed, The Easy Way!46Array[(lnt, String}] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :b.reduceByKey(_ + _).collectreduceByKey JPair] : This function provides the well-known reduce functionality in Spark.Please note that any function f you provide, should be commutative in order to generatereproducible results.NO.56 CORRECT TEXTProblem Scenario 52 : You have been given below code snippet.val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))Operation_xyzWrite a correct code snippet for Operation_xyz which will produce below output.scalaxollection.Map[lnt,Long] = Map(5 -> 1, 8 -> 1, 3 -> 1, 6 -> 1, 1 -> S, 2 -> 3, 4 -> 2, 7 ->1)Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :b.countByValuecountByValueReturns a map that contains all unique values of the RDD and their respective occurrence counts.(Warning: This operation will finally aggregate the information in a single reducer.)Listing Variantsdef countByValue(): Map[T, Long]NO.57 CORRECT TEXTProblem Scenario 23 : You have been given log generating service as below.Start_logs (It will generate continuous logs)Tail_logs (You can check , what logs are being generated)Stop_logs (It will stop the log service)Path where logs are generated using above service : /opt/gen_logs/logs/access.logNow write a flume configuration file named flume3.conf , using that configuration file dumps logs inHDFS file system in a directory called flumeflume3/%Y/%m/%d/%H/%MMeans every minute new directory should be created). Please us the interceptors to providetimestamp information, if message header does not have header info.And also note that you have to preserve existing timestamp, if message contains it. Flume channelshould have following property as well. After every 100 message it should be committed, use non-durable/faster channel and it should be able to hold maximum 1000 events.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create flume configuration file, with below configuration for source, sink and channel.IT Certification Guaranteed, The Easy Way!47#Define source , sink , channel and agent,agent1 .sources = source1agent1 .sinks = sink1agent1.channels = channel1# Describe/configure source1agent1 .sources.source1.type = execagentl.sources.source1.command = tail -F /opt/gen logs/logs/access.log#Define interceptorsagent1 .sources.source1.interceptors=i1agent1 .sources.source1.interceptors.i1.type=timestampagent1 .sources.source1.interceptors.i1.preserveExisting=true## Describe sink1agent1 .sinks.sink1.channel = memory-channelagent1 .sinks.sink1.type = hdfsagent1 .sinks.sink1.hdfs.path = flume3/%Y/%m/%d/%H/%Magent1 .sinks.sjnkl.hdfs.fileType = Data Stream# Now we need to define channel1 property.agent1.channels.channel1.type = memoryagent1.channels.channel1.capacity = 1000agent1.channels.channel1.transactionCapacity = 100# Bind the source and sink to the channelAgent1.sources.source1.channels = channel1agent1.sinks.sink1.channel = channel1Step 2 : Run below command which will use this configuration file and append data in hdfs.Start log service using : start_logsStart flume service:flume-ng agent -conf /home/cloudera/flumeconf -conf-file/home/cloudera/flumeconf/flume3.conf -DfIume.root.logger=DEBUG,INFO,console -name agent1Wait for few mins and than stop log service.stop logsNO.58 CORRECT TEXTProblem Scenario 33 : You have given a files as below.spark5/EmployeeName.csv (id,name)spark5/EmployeeSalary.csv (id,salary)Data is given below:EmployeeName.csvE01,LokeshE02,BhupeshE03,AmitE04,RatanE05,DineshE06,PavanE07,TejasE08,SheelaE09,KumarIT Certification Guaranteed, The Easy Way!48E10,VenkatEmployeeSalary.csvE01,50000E02,50000E03,45000E04,45000E05,50000E06,45000E07,50000E08,10000E09,10000E10,10000Now write a Spark code in scala which will load these two tiles from hdfs and join the same, andproduce the (name.salary) values.And save the data in multiple tile group by salary (Means each file will have name of employees withsame salary). Make sure file name include salary as well.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create all three files in hdfs (We will do using Hue). However, you can first create in localfilesystem and then upload it to hdfs.Step 2 : Load EmployeeName.csv file from hdfs and create PairRDDsval name = sc.textFile("spark5/EmployeeName.csv")val namePairRDD = name.map(x=> (x.split(",")(0),x.split('V')(1)))Step 3 : Load EmployeeSalary.csv file from hdfs and create PairRDDsval salary = sc.textFile("spark5/EmployeeSalary.csv")val salaryPairRDD = salary.map(x=> (x.split(",")(0),x.split(",")(1)))Step 4 : Join all pairRDDSval joined = namePairRDD.join(salaryPairRDD}Step 5 : Remove key from RDD and Salary as a Key. val keyRemoved = joined.valuesStep 6 : Now swap filtered RDD.val swapped = keyRemoved.map(item => item.swap)Step 7 : Now groupBy keys (It will generate key and value array) val grpByKey =swapped.groupByKey().collect()Step 8 : Now create RDD for values collectionval rddByKey = grpByKey.map{case (k,v) => k->sc.makeRDD(v.toSeq)}Step 9 : Save the output as a Text file.rddByKey.foreach{ case (k,rdd) => rdd.saveAsTextFile("spark5/Employee"+k)}NO.59 CORRECT TEXTProblem Scenario 55 : You have been given below code snippet.val pairRDDI = sc.parallelize(List( ("cat",2), ("cat", 5), ("book", 4),("cat", 12))) val pairRDD2 =sc.parallelize(List( ("cat",2), ("cup", 5), ("mouse", 4),("cat", 12))) operation1Write a correct code snippet for operationl which will produce desired output, shown below.Array[(String, (Option[lnt], Option[lnt]))] = Array((book,(Some(4},None)),IT Certification Guaranteed, The Easy Way!49(mouse,(None,Some(4))), (cup,(None,Some(5))), (cat,(Some(2),Some(2)),(cat,(Some(2),Some(12))), (cat,(Some(5),Some(2))), (cat,(Some(5),Some(12))),(cat,(Some(12),Some(2))), (cat,(Some(12),Some(12)))JAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution : pairRDD1.fullOuterJoin(pairRDD2).collectfullOuterJoin [Pair]Performs the full outer join between two paired RDDs.Listing Variantsdef fullOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Option[V],OptionfW]))]def fullOuterJoin[W](other: RDD[(K, W}]}: RDD[(K, (Option[V], OptionfW]))] deffullOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Option[V],Option[W]))]NO.60 CORRECT TEXTProblem Scenario 34 : You have given a file named spark6/user.csv.Data is given below:user.csvid,topic,hitsRahul,scala,120Nikita,spark,80Mithun,spark,1myself,cca175,180Now write a Spark code in scala which will remove the header part and create RDD of values asbelow, for all rows. And also if id is myself" than filter out row.Map(id -> om, topic -> scala, hits -> 120)Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create file in hdfs (We will do using Hue). However, you can first create in local filesystemand then upload it to hdfs.Step 2 : Load user.csv file from hdfs and create PairRDDs val csv =sc.textFile("spark6/user.csv")Step 3 : split and clean dataval headerAndRows = csv.map(line => line.split(",").map(_.trim))Step 4 : Get header rowval header = headerAndRows.firstStep 5 : Filter out header (We need to check if the first val matches the first header name) val data =headerAndRows.filter(_(0) != header(O))Step 6 : Splits to map (header/value pairs)val maps = data.map(splits => header.zip(splits).toMap)step 7: Filter out the user "myselfval result = maps.filter(map => mapf'id") != "myself")IT Certification Guaranteed, The Easy Way!50Step 8 : Save the output as a Text file. result.saveAsTextFile("spark6/result.txt")NO.61 CORRECT TEXTProblem Scenario 43 : You have been given following code snippet.val grouped = sc.parallelize(Seq(((1,"twoM), List((3,4), (5,6)))))val flattened = grouped.flatMap {A =>groupValues.map { value => B }}You need to generate following output.Hence replace A and BArray((1,two,3,4),(1,two,5,6))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :A case (key, groupValues)B (key._1, key._2, value._1, value._2)NO.62 CORRECT TEXTProblem Scenario 25 : You have been given below comma separated employee information. Thatneeds to be added in /home/cloudera/flumetest/in.txt file (to do tail source) sex,name,city1 ,alok,mumbai1 ,jatin,chennai1 ,yogesh,kolkata2 ,ragini,delhi2 ,jyotsana,pune1,valmiki,bangloreCreate a flume conf file using fastest non-durable channel, which write data in hive warehousedirectory, in two separate tables called flumemaleemployee1 and flumefemaleemployee1(Create hive table as well for given data}. Please use tail source with/home/cloudera/flumetest/in.txt file.Flumemaleemployee1 : will contain only male employees data flumefemaleemployee1 :Will contain only woman employees dataAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create hive table for flumemaleemployeel and .'CREATE TABLE flumemaleemployeel(sex_type int, name string, city string )ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';CREATE TABLE flumefemaleemployeel(sex_type int, name string, city string)IT Certification Guaranteed, The Easy Way!51ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';Step 2 : Create below directory and file mkdir /home/cloudera/flumetest/ cd/home/cloudera/flumetest/Step 3 : Create flume configuration file, with below configuration for source, sink and channel andsave it in flume5.conf.agent.sources = tailsrcagent.channels = mem1 mem2agent.sinks = stdl std2agent.sources.tailsrc.type = execagent.sources.tailsrc.command = tail -F /home/cloudera/flumetest/in.txtagent.sources.tailsrc.batchSize = 1 agent.sources.tailsrc.interceptors = i1agent.sources.tailsrc.interceptors.i1.type = regex_extractor agent.sources.tailsrc.interceptors.il.regex= A(\\d} agent.sources.tailsrc.interceptors. M.serializers = t1 agent.sources.tailsrc. interceptors, i1.serializers.t1. name = typeagent.sources.tailsrc.selector.type = multiplexing agent.sources.tailsrc.selector.header = typeagent.sources.tailsrc.selector.mapping.1 = memi agent.sources.tailsrc.selector.mapping.2 = mem2agent.sinks.std1.type = hdfsagent.sinks.stdl.channel = mem1agent.sinks.stdl.batchSize = 1agent.sinks.std1.hdfs.path = /user/hive/warehouse/flumemaleemployeeiagent.sinks.stdl.rolllnterval = 0agent.sinks.stdl.hdfs.tileType = Data Streamagent.sinks.std2.type = hdfsagent.sinks.std2.channel = mem2agent.sinks.std2.batchSize = 1agent.sinks.std2.hdfs.path = /user/hi ve/warehouse/fIumefemaleemployee1agent.sinks.std2.rolllnterval = 0 agent.sinks.std2.hdfs.tileType = Data Streamagent.channels.mem1.type = memory agent.channels.meml.capacity = 100agent.channels.mem2.type = memory agent.channels.mem2.capacity = 100agent.sources.tailsrc.channels = mem1 mem2Step 4 : Run below command which will use this configuration file and append data in hdfs.Start flume service:flume-ng agent -conf /home/cloudera/flumeconf -conf-file/home/cloudera/fIumeconf/flume5.conf --name agentStep 5 : Open another terminal create a file at /home/cloudera/flumetest/in.txt.Step 6 : Enter below data in file and save it.l.alok.mumbai1 jatin.chennai1 ,yogesh,kolkata2 ,ragini,delhi2 ,jyotsana,pune1,valmiki,bangloreStep 7 : Open hue and check the data is available in hive table or not.Step 8 : Stop flume service by pressing ctrl+cNO.63 CORRECT TEXTIT Certification Guaranteed, The Easy Way!52Problem Scenario 78 : You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.orderstable=retail_db.order_itemsjdbc URL = jdbc:mysql://quickstart:3306/retail_dbColumns of order table : (orderid , order_date , order_customer_id, order_status)Columns of ordeMtems table : (order_item_td , order_item_order_id ,order_item_product_id,order_item_quantity,order_item_subtotal,order_item_product_price)Please accomplish following activities.1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directoryp92_orders and p92_order_items .2. Join these data using order_id in Spark and Python3. Calculate total revenue perday and per customer4. Calculate maximum revenue customerAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Import Single table .sqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -password=cloudera -table=orders --target-dir=p92_orders -m 1 sqoop import -connectjdbc:mysql://quickstart:3306/retail_db -username=retail_dba - password=cloudera-table=order_items --target-dir=p92_order_orderitems --m 1Note : Please check you dont have space between before or after '=' sign. Sqoop uses theMapReduce framework to copy data from RDBMS to hdfsStep 2 : Read the data from one of the partition, created using above command, hadoop fs-cat p92_orders/part-m-00000 hadoop fs -cat p92 orderitems/part-m-00000Step 3 : Load these above two directory as RDD using Spark and Python (Open pyspark terminal anddo following). orders = sc.textFile(Mp92_orders") orderitems = sc.textFile("p92_order_items")Step 4 : Convert RDD into key value as (orderjd as a key and rest of the values as a value)#First value is orderjdorders Key Value = orders.map(lambda line: (int(line.split(",")[0]), line))#Second value as an OrderjdorderltemsKeyValue = orderltems.map(lambda line: (int(line.split(",")[1]), line))Step 5 : Join both the RDD using orderjdjoinedData = orderltemsKeyValue.join(ordersKeyValue)#print the joined datafor line in joinedData.collect():print(line)#Format of joinedData as below.#[Orderld, 'All columns from orderltemsKeyValue', 'All columns from ordersKeyValue']ordersPerDatePerCustomer = joinedData.map(lambda line: ((line[1][1].split(",")[1],line[1][1].split(",M)[2]), float(line[1][0].split(",")[4]))) amountCollectedPerDayPerCustomer =IT Certification Guaranteed, The Easy Way!53ordersPerDatePerCustomer.reduceByKey(lambda runningSum, amount: runningSum + amount}#(Out record format will be ((date,customer_id), totalAmount} for line inamountCollectedPerDayPerCustomer.collect(): print(line)#now change the format of record as (date,(customer_id,total_amount))revenuePerDatePerCustomerRDD = amountCollectedPerDayPerCustomer.map(lambdathreeElementTuple: (threeElementTuple[0][0],(threeElementTuple[0][1],threeElementTuple[1])))for line in revenuePerDatePerCustomerRDD.collect():print(line)#Calculate maximum amount collected by a customer for each dayperDateMaxAmountCollectedByCustomer =revenuePerDatePerCustomerRDD.reduceByKey(lambda runningAmountTuple,newAmountTuple: (runningAmountTuple if runningAmountTuple[1] >=newAmountTuple[1] else newAmountTuple})for line in perDateMaxAmountCollectedByCustomer\sortByKey().collect(): print(line)NO.64 CORRECT TEXTProblem Scenario 74 : You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.orderstable=retail_db.order_itemsjdbc URL = jdbc:mysql://quickstart:3306/retail_dbColumns of order table : (orderjd , order_date , ordercustomerid, order status}Columns of orderjtems table : (order_item_td , order_item_order_id ,order_item_product_id,order_item_quantity,order_item_subtotal,order_item_product_price)Please accomplish following activities.1. Copy "retaildb.orders" and "retaildb.orderjtems" table to hdfs in respective directory p89_ordersand p89_order_items .2. Join these data using orderjd in Spark and Python3. Now fetch selected columns from joined data Orderld, Order date and amount collected on thisorder.4. Calculate total order placed for each date, and produced the output sorted by date.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution:Step 1 : Import Single table .sqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -password=cloudera -table=orders --target-dir=p89_orders - -m1 sqoop import --connectjdbc:mysql://quickstart:3306/retail_db -username=retail_dba - password=cloudera-table=order_items ~target-dir=p89_ order items -m 1Note : Please check you dont have space between before or after '=' sign. Sqoop uses theMapReduce framework to copy data from RDBMS to hdfsIT Certification Guaranteed, The Easy Way!54Step 2 : Read the data from one of the partition, created using above command, hadoopfs-cat p89_orders/part-m-00000 hadoop fs -cat p89_order_items/part-m-00000Step 3 : Load these above two directory as RDD using Spark and Python (Open pyspark terminal anddo following). orders = sc.textFile("p89_orders") orderitems = sc.textFile("p89_order_items")Step 4 : Convert RDD into key value as (orderjd as a key and rest of the values as a value)#First value is orderjdordersKeyValue = orders.map(lambda line: (int(line.split(",")[0]), line))#Second value as an OrderjdorderltemsKeyValue = orderltems.map(lambda line: (int(line.split(",")[1]), line))Step 5 : Join both the RDD using orderjdjoinedData = orderltemsKeyValue.join(ordersKeyValue)#print the joined datator line in joinedData.collect():print(line)Format of joinedData as below.[Orderld, 'All columns from orderltemsKeyValue', 'All columns from orders Key Value']Step 6 : Now fetch selected values Orderld, Order date and amount collected on this order.revenuePerOrderPerDay = joinedData.map(lambda row: (row[0]( row[1][1].split(",")[1](f!oat(row[1][0].split('\M}[4]}}}#printthe resultfor line in revenuePerOrderPerDay.collect():print(line)Step 7 : Select distinct order ids for each date.#distinct(date,order_id)distinctOrdersDate = joinedData.map(lambda row: row[1][1].split('\")[1] + "," + str(row[0])).distinct()for line in distinctOrdersDate.collect(): print(line)Step 8 : Similar to word count, generate (date, 1) record for each row. newLineTuple =distinctOrdersDate.map(lambda line: (line.split(",")[0], 1))Step 9 : Do the count for each key(date), to get total order per date. totalOrdersPerDate =newLineTuple.reduceByKey(lambda a, b: a + b}#print resultsfor line in totalOrdersPerDate.collect():print(line)step 10 : Sort the results by date sortedData=totalOrdersPerDate.sortByKey().collect()#print resultsfor line in sortedData:print(line)NO.65 CORRECT TEXTProblem Scenario 72 : You have been given a table named "employee2" with following detail.first_name stringlast_name stringWrite a spark script in python which read this table and print all the rows and individual columnvalues.Answer:See the explanation for Step by Step Solution and configuration.IT Certification Guaranteed, The Easy Way!55Explanation:Solution :Step 1 : Import statements for HiveContext from pyspark.sql import HiveContextStep 2 : Create sqIContext sqIContext = HiveContext(sc)Step 3 : Query hiveemployee2 = sqlContext.sql("select' from employee2")Step 4 : Now prints the data for row in employee2.collect(): print(row)Step 5 : Print specific column for row in employee2.collect(): print( row.fi rst_name)NO.66 CORRECT TEXTProblem Scenario 9 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following.1. Import departments table in a directory.2. Again import departments table same directory (However, directory already exist hence it shouldnot overrride and append the results)3. Also make sure your results fields are terminated by '|' and lines terminated by '\n\Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solutions :Step 1 : Clean the hdfs file system, if they exists clean out.hadoop fs -rm -R departmentshadoop fs -rm -R categorieshadoop fs -rm -R productshadoop fs -rm -R ordershadoop fs -rm -R order_itemshadoop fs -rm -R customersStep 2 : Now import the department table as per requirement.sqoop import \-connect jdbc:mysql://quickstart:330G/retaiI_db \--username=retail_dba \-password=cloudera \-table departments \-target-dir=departments \-fields-terminated-by '|' \-lines-terminated-by '\n' \-mlStep 3 : Check imported data.hdfs dfs -Is departmentshdfs dfs -cat departments/part-m-00000Step 4 : Now again import data and needs to appended.sqoop import \IT Certification Guaranteed, The Easy Way!56-connect jdbc:mysql://quickstart:3306/retail_db \--username=retail_dba \-password=cloudera \-table departments \-target-dir departments \-append \-tields-terminated-by '|' \-lines-termtnated-by '\n' \-mlStep 5 : Again Check the resultshdfs dfs -Is departmentshdfs dfs -cat departments/part-m-00001NO.67 CORRECT TEXTProblem Scenario 44 : You have been given 4 files , with the content as given below:spark11/file1.txtApache Hadoop is an open-source software framework written in Java for distributed storage anddistributed processing of very large data sets on computer clusters built from commodity hardware.All the modules in Hadoop are designed with a fundamental assumption that hardware failures arecommon and should be automatically handled by the framework spark11/file2.txtThe core of Apache Hadoop consists of a storage part known as Hadoop Distributed FileSystem (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks anddistributes them across nodes in a cluster. To process data, Hadoop transfers packaged code fornodes to process in parallel based on the data that needs to be processed.spark11/file3.txthis approach takes advantage of data locality nodes manipulating the data they have access to toallow the dataset to be processed faster and more efficiently than it would be in a more conventionalsupercomputer architecture that relies on a parallel file system where computation and data aredistributed via high-speed networking spark11/file4.txtApache Storm is focused on stream processing or what some call complex event processing. Stormimplements a fault tolerant method for performing a computation or pipelining multiplecomputations on an event as it flows into a system. One might useStorm to transform unstructured data as it flows into a system into a desired format(spark11Afile1.txt)(spark11/file2.txt)(spark11/file3.txt)(sparkl 1/file4.txt)Write a Spark program, which will give you the highest occurring words in each file. With their filename and highest occurring words.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create all 4 file first using Hue in hdfs.Step 2 : Load all file as an RDDval file1 = sc.textFile("sparkl1/filel.txt")IT Certification Guaranteed, The Easy Way!57val file2 = sc.textFile("spark11/file2.txt")val file3 = sc.textFile("spark11/file3.txt")val file4 = sc.textFile("spark11/file4.txt")Step 3 : Now do the word count for each file and sort in reverse order of count.val contentl = filel.flatMap( line => line.split(" ")).map(word => (word,1)).reduceByKey(_ +_).map(item => item.swap).sortByKey(false).map(e=>e.swap)val content.2 = file2.flatMap( line => line.splitf ")).map(word => (word,1)).reduceByKey(_+ _).map(item => item.swap).sortByKey(false).map(e=>e.swap)val content3 = file3.flatMap( line > line.split)" ")).map(word => (word,1)).reduceByKey(_+ _).map(item => item.swap).sortByKey(false).map(e=>e.swap)val content4 = file4.flatMap( line => line.split(" ")).map(word => (word,1)).reduceByKey(_ +_ ).map(item => item.swap).sortByKey(false).map(e=>e.swap)Step 4 : Split the data and create RDD of all Employee objects.val filelword = sc.makeRDD(Array(file1.name+"->"+content1(0)._1+"-"+content1(0)._2)) val file2word= sc.makeRDD(Array(file2.name+"->"+content2(0)._1+"-"+content2(0)._2)) val file3word =sc.makeRDD(Array(file3.name+"->"+content3(0)._1+"-"+content3(0)._2)) val file4word =sc.makeRDD(Array(file4.name+M->"+content4(0)._1+"-"+content4(0)._2))Step 5: Union all the RDDSval unionRDDs = filelword.union(file2word).union(file3word).union(file4word)Step 6 : Save the results in a text file as below.unionRDDs.repartition(1).saveAsTextFile("spark11/union.txt")NO.68 CORRECT TEXTProblem Scenario 15 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following activities.1. In mysql departments table please insert following record. Insert into departments values(9999,'"Data Science"1);2. Now there is a downstream system which will process dumps of this file. However, system isdesigned the way that it can process only files if fields are enlcosed in(') single quote and separate ofthe field should be (-} and line needs to be terminated by : (colon).3. If data itself contains the " (double quote } than it should be escaped by \.4. Please import the departments table in a directory called departments_enclosedby and file shouldbe able to process by downstream system.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Connect to mysql database.mysql --user=retail_dba -password=clouderashow databases; use retail_db; show tables;Insert recordInsert into departments values(9999, '"Data Science"');IT Certification Guaranteed, The Easy Way!58select" from departments;Step 2 : Import data as per requirement.sqoop import \-connect jdbc:mysql;//quickstart:3306/retail_db \~ username=retail_dba \--password=cloudera \-table departments \-target-dir /user/cloudera/departments_enclosedby \-enclosed-by V -escaped-by \\ -fields-terminated-by--' -lines-terminated-by :Step 3 : Check the result.hdfs dfs -cat/user/cloudera/departments_enclosedby/part"NO.69 CORRECT TEXTProblem Scenario 86 : In Continuation of previous question, please accomplish following activities.1 . Select Maximum, minimum, average , Standard Deviation, and total quantity.2 . Select minimum and maximum price for each product code.3. Select Maximum, minimum, average , Standard Deviation, and total quantity for each productcode, hwoever make sure Average and Standard deviation will have maximum two decimal values.4. Select all the product code and average price only where product count is more than or equal to 3.5. Select maximum, minimum , average and total of all the products for each code. Also produce thesame across all the products.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Select Maximum, minimum, average , Standard Deviation, and total quantity.val results = sqlContext.sql('.....SELECT MAX(price) AS MAX , MIN(price) AS MIN ,AVG(price) AS Average, STD(price) AS STD, SUM(quantity) AS total_products FROM products......)results. showQStep 2 : Select minimum and maximum price for each product code.val results = sqlContext.sql(......SELECT code, MAX(price) AS Highest Price', MIN(price)AS Lowest Price'FROM products GROUP BY code......)results. showQStep 3 : Select Maximum, minimum, average , Standard Deviation, and total quantity for eachproduct code, hwoever make sure Average and Standard deviation will have maximum two decimalvalues.val results = sqlContext.sql(......SELECT code, MAX(price), MIN(price),CAST(AVG(price} AS DECIMAL(7,2)) AS Average', CAST(STD(price) AS DECIMAL(7,2))AS 'Std Dev\ SUM(quantity) FROM productsGROUP BY code......)results. showQStep 4 : Select all the product code and average price only where product count is more than or equalto 3.val results = sqlContext.sql(......SELECT code AS Product Code',COUNTf) AS Count',IT Certification Guaranteed, The Easy Way!59CAST(AVG(price) AS DECIMAL(7,2)) AS Average' FROM products GROUP BY codeHAVING Count >=3"M") results. showQStep 5 : Select maximum, minimum , average and total of all the products for each code.Also produce the same across all the products.val results = sqlContext.sql( """SELECTcode,MAX(price),MIN(pnce),CAST(AVG(price) AS DECIMAL(7,2)) AS Average',SUM(quantity)-FROM productsGROUP BY codeWITH ROLLUP""" )results. show()NO.70 CORRECT TEXTProblem Scenario 3: You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.categoriesjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following activities.1. Import data from categories table, where category=22 (Data should be stored in categories subset)2. Import data from categories table, where category>22 (Data should be stored incategories_subset_2)3. Import data from categories table, where category between 1 and 22 (Data should be stored incategories_subset_3)4. While importing catagories data change the delimiter to '|' (Data should be stored incategories_subset_S)5. Importing data from catagories table and restrict the import to category_name,category idcolumns only with delimiter as '|'6 . Add null values in the table using below SQL statement ALTER TABLE categories modifycategory_department_id int(11); INSERT INTO categories values(eO.NULL.'TESTING');7. Importing data from catagories table (In categories_subset_17 directory) using '|' delimiter andcategoryjd between 1 and 61 and encode null values for both string and non string columns.8. Import entire schema retail_db in a directory categories_subset_all_tablesAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution:Step 1: Import Single table (Subset data} Note: Here the ' is the same you find on - key sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba - password=cloudera-table=categories ~warehouse-dir= categories_subset --where\'category_id\'=22 --m 1IT Certification Guaranteed, The Easy Way!60Step 2 : Check the output partitionhdfs dfs -cat categoriessubset/categories/part-m-00000Step 3 : Change the selection criteria (Subset data)sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba -password=cloudera -table=categories ~warehouse-dir= categories_subset_2 --where\'category_id\'\>22 -m 1Step 4 : Check the output partitionhdfs dfs -cat categories_subset_2/categories/part-m-00000Step 5 : Use between clause (Subset data)sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba -password=cloudera -table=categories ~warehouse-dir=categories_subset_3 --where"\'category_id\' between 1 and 22" --m 1Step 6 : Check the output partitionhdfs dfs -cat categories_subset_3/categories/part-m-00000Step 7 : Changing the delimiter during import.sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail dba -password=cloudera -table=categories -warehouse-dir=:categories_subset_6 --where"/'categoryjd /' between 1 and 22" -fields-terminated-by='|' -m 1Step 8 : Check the.output partitionhdfs dfs -cat categories_subset_6/categories/part-m-00000Step 9 : Selecting subset columnssqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba -password=cloudera -table=categories --warehouse-dir=categories subset col -where"/'category id/' between 1 and 22" -fields-terminated-by=T -columns=category name,category id --m1Step 10 : Check the output partitionhdfs dfs -cat categories_subset_col/categories/part-m-00000Step 11 : Inserting record with null values (Using mysql} ALTER TABLE categories modifycategory_department_id int(11); INSERT INTO categories values ^NULL/TESTING'); select" fromcategories;Step 12 : Encode non string null columnsqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail dba -password=cloudera -table=categories --warehouse-dir=categortes_subset_17 -where"\"category_id\" between 1 and 61" -fields-terminated-by=,|' --null-string-N' -null-non- string=,N' --m1Step 13 : View the contenthdfs dfs -cat categories_subset_17/categories/part-m-00000Step 14 : Import all the tables from a schema (This step will take little time) sqoop import-all-tables -connect jdbc:mysql://quickstart:3306/retail_db -- username=retail_dba -password=cloudera-warehouse-dir=categories_siStep 15 : View the contentshdfs dfs -Is categories_subset_all_tablesStep 16 : Cleanup or back to originals.delete from categories where categoryid in (59,60);ALTER TABLE categories modify category_department_id int(11) NOTNULL;ALTER TABLE categories modify category_name varchar(45) NOT NULL;IT Certification Guaranteed, The Easy Way!61desc categories;NO.71 CORRECT TEXTProblem Scenario 87 : You have been given below three filesproduct.csv (Create this file in hdfs)productID,productCode,name,quantity,price,supplierid1 001,PEN,Pen Red,5000,1.23,5011 002,PEN,Pen Blue,8000,1.25,5011003,PEN,Pen Black,2000,1.25,5011004,PEC,Pencil 2B,10000,0.48,5021005,PEC,Pencil 2H,8000,0.49,5021006,PEC,Pencil HB,0,9999.99,5022001,PEC,Pencil 3B,500,0.52,5012002,PEC,Pencil 4B,200,0.62,5012003,PEC,Pencil 5B,100,0.73,5012004,PEC,Pencil 6B,500,0.47,502supplier.csvsupplierid,name,phone501,ABC Traders,88881111502,XYZ Company,88882222503,QQ Corp,88883333products_suppliers.csvproductID,supplierID2001,5012002,5012003,5012004,5022001,503Now accomplish all the queries given in solution.Select product, its price , its supplier name where product price is less than 0.6 usingSparkSQLAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1:hdfs dfs -mkdir sparksql2hdfs dfs -put product.csv sparksq!2/hdfs dfs -put supplier.csv sparksql2/hdfs dfs -put products_suppliers.csv sparksql2/Step 2 : Now in spark shell// this Is used to Implicitly convert an RDD to a DataFrame.import sqlContext.impIicits._// Import Spark SQL data types and Row.import org.apache.spark.sql._// load the data into a new RDDIT Certification Guaranteed, The Easy Way!62val products = sc.textFile("sparksql2/product.csv")val supplier = sc.textFileC'sparksq^supplier.csv")val prdsup = sc.textFile("sparksql2/products_suppliers.csv"}// Return the first element in this RDDproducts.fi rst()supplier.first{).prdsup.first()//define the schema using a case classcase class Product(productid: Integer, code: String, name: String, quantity:lnteger, price:Float, supplierid:lnteger)case class Suplier(supplierid: Integer, name: String, phone: String)case class PRDSUP(productid: Integer.supplierid: Integer)// create an RDD of Product objectsval prdRDD = products.map(_.split('\")).map(p =>Product(p(0).tolnt,p(1),p(2),p(3).tolnt,p(4).toFloat,p(5).toint))val supRDD = supplier.map(_.split(",")).map(p => Suplier(p(0).tolnt,p(1),p(2))) val prdsupRDD =prdsup.map(_.split(",")).map(p => PRDSUP(p(0).tolnt,p(1}.tolnt}} prdRDD.first() prdRDD.count()supRDD.first() supRDD.count()prdsupRDD.first() prdsupRDD.count(}// change RDD of Product objects to a DataFrameval prdDF = prdRDD.toDF()val supDF = supRDD.toDF()val prdsupDF = prdsupRDD.toDF()// register the DataFrame as a temp table prdDF.registerTempTablef'products")supDF.registerTempTablef'suppliers") prdsupDF.registerTempTablef'productssuppliers"}//Select product, its price , its supplier name where product price is less than 0.6 val results =sqlContext.sql(......SELECT products.name, price, suppliers.name as sup_name FROM products JOINsuppliers ON products.supplierlD= suppliers.supplierlDWHERE price < 0.6......]results. show()NO.72 CORRECT TEXTProblem Scenario 19 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbNow accomplish following activities.1. Import departments table from mysql to hdfs as textfile in departments_text directory.2. Import departments table from mysql to hdfs as sequncefile in departments_sequence directory.3. Import departments table from mysql to hdfs as avro file in departments avro directory.4. Import departments table from mysql to hdfs as parquet file in departments_parquet directory.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :IT Certification Guaranteed, The Easy Way!63Step 1 : Import departments table from mysql to hdfs as textfilesqoop import \-connect jdbc:mysql://quickstart:3306/retail_db \~ username=retail_dba \-password=cloudera \-table departments \-as-textfile \-target-dir=departments_textverify imported datahdfs dfs -cat departments_text/part"Step 2 : Import departments table from mysql to hdfs as sequncetllesqoop import \-connect jdbc:mysql://quickstart:330G/retaiI_db \~ username=retail_dba \-password=cloudera \--table departments \-as-sequencetlle \-~target-dir=departments sequenceverify imported datahdfs dfs -cat departments_sequence/part*Step 3 : Import departments table from mysql to hdfs as sequncetllesqoop import \-connect jdbc:mysql://quickstart:330G/retaiI_db \~ username=retail_dba \--password=cloudera \--table departments \--as-avrodatafile \--target-dir=departments_avroverify imported datahdfs dfs -cat departments avro/part*Step 4 : Import departments table from mysql to hdfs as sequncetllesqoop import \-connect jdbc:mysql://quickstart:330G/retaiI_db \~ username=retail_dba \--password=cloudera \-table departments \-as-parquetfile \-target-dir=departments_parquetverify imported datahdfs dfs -cat departmentsparquet/part*NO.73 CORRECT TEXTProblem Scenario 82 : You have been given table in Hive with following structure (Which you havecreated in previous exercise).productid int code string name string quantity int price floatUsing SparkSQL accomplish following activities.IT Certification Guaranteed, The Easy Way!641 . Select all the products name and quantity having quantity <= 20002 . Select name and price of the product having code as 'PEN'3 . Select all the products, which name starts with PENCIL4 . Select all products which "name" begins with 'P\ followed by any two characters, followed byspace, followed by zero or more charactersAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Copy following tile (Mandatory Step in Cloudera QuickVM) if you have not done it.sudo su rootcp /usr/lib/hive/conf/hive-site.xml /usr/lib/sparkVconf/Step 2 : Now start spark-shellStep 3 ; Select all the products name and quantity having quantity <= 2000 val results =sqlContext.sql(......SELECT name, quantity FROM products WHERE quantity< = 2000......)results.showQStep 4 : Select name and price of the product having code as 'PEN'val results = sqlContext.sql(......SELECT name, price FROM products WHERE code ='PEN.......)results. showQStep 5 : Select all the products , which name starts with PENCILval results = sqlContext.sql(......SELECT name, price FROM products WHERE upper(name) LIKE'PENCIL%.......} results. showQStep 6 : select all products which "name" begins with 'P', followed by any two characters, followed byspace, followed byzero or more characters-- "name" begins with 'P', followed by any two characters,- followed by space, followed by zero or more charactersval results = sqlContext.sql(......SELECT name, price FROM products WHERE name LIKE'P_ %.......)results. show()NO.74 CORRECT TEXTProblem Scenario 18 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbNow accomplish following activities.1. Create mysql table as below.mysql --user=retail_dba -password=clouderause retail_dbCREATE TABLE IF NOT EXISTS departments_hive02(id int, department_namevarchar(45), avg_salary int);show tables;2. Now export data from hive table departments_hive01 in departments_hive02. While exporting,IT Certification Guaranteed, The Easy Way!65please note following. wherever there is a empty string it should be loaded as a null value in mysql.wherever there is -999 value for int field, it should be created as null value.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create table in mysql db as well.mysql ~user=retail_dba -password=clouderause retail_dbCREATE TABLE IF NOT EXISTS departments_hive02(id int, department_namevarchar(45), avg_salary int);show tables;Step 2 : Now export data from hive table to mysql table as per the requirement.sqoop export --connect jdbc:mysql://quickstart:3306/retail_db \-username retaildba \-password cloudera \--table departments_hive02 \-export-dir /user/hive/warehouse/departments_hive01 \-input-fields-terminated-by '\001' \--input-Iines-terminated-by '\n' \--num-mappers 1 \-batch \-Input-null-string "" \-input-null-non-string -999step 3 : Now validate the data,select * from departments_hive02;NO.75 CORRECT TEXTProblem Scenario 2 :There is a parent organization called "ABC Group Inc", which has two child companies named TechInc and MPTech.Both companies employee information is given in two separate text file as below. Please do thefollowing activity for employee details.Tech Inc.txt1,Alok,Hyderabad2,Krish,Hongkong3,Jyoti,Mumbai4 ,Atul,Banglore5 ,Ishan,GurgaonMPTech.txt6 ,John,Newyork7 ,alp2004,California8 ,tellme,Mumbai9 ,Gagan21,Pune1 0,Mukesh,Chennai1 . Which command will you use to check all the available command line options on HDFS and Howwill you get the Help for individual command.IT Certification Guaranteed, The Easy Way!662. Create a new Empty Directory named Employee using Command line. And also create an empty filenamed in it Techinc.txt3. Load both companies Employee data in Employee directory (How to override existing file in HDFS).4. Merge both the Employees data in a Single tile called MergedEmployee.txt, merged tiles shouldhave new line character at the end of each file content.5. Upload merged file on HDFS and change the file permission on HDFS merged file, so that ownerand group member can read and write, other user can read the file.6. Write a command to export the individual file as well as entire directory from HDFS to local fileSystem.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Check All Available command hdfs dfsStep 2 : Get help on Individual command hdfs dfs -help getStep 3 : Create a directory in HDFS using named Employee and create a Dummy file in it called e.g.Techinc.txt hdfs dfs -mkdir EmployeeNow create an emplty file in Employee directory using Hue.Step 4 : Create a directory on Local file System and then Create two files, with the given data inproblems.Step 5 : Now we have an existing directory with content in it, now using HDFS command line , overridthis existing Employee directory. While copying these files from local fileSystem to HDFS. cd /home/cloudera/Desktop/ hdfs dfs -put -f EmployeeStep 6 : Check All files in directory copied successfully hdfs dfs -Is EmployeeStep 7 : Now merge all the files in Employee directory, hdfs dfs -getmerge -nl EmployeeMergedEmployee.txtStep 8 : Check the content of the file. cat MergedEmployee.txtStep 9 : Copy merged file in Employeed directory from local file ssytem to HDFS. hdfs dfs - putMergedEmployee.txt Employee/Step 10 : Check file copied or not. hdfs dfs -Is EmployeeStep 11 : Change the permission of the merged file on HDFS hdfs dfs -chmpd 664Employee/MergedEmployee.txtStep 12 : Get the file from HDFS to local file system, hdfs dfs -get EmployeeEmployee_hdfsNO.76 CORRECT TEXTProblem Scenario 27 : You need to implement near real time solutions for collecting informationwhen submitted in file with below information.Dataecho "IBM,100,20160104" >> /tmp/spooldir/bb/.bb.txtecho "IBM,103,20160105" >> /tmp/spooldir/bb/.bb.txtmv /tmp/spooldir/bb/.bb.txt /tmp/spooldir/bb/bb.txtAfter few minsecho "IBM,100.2,20160104" >> /tmp/spooldir/dr/.dr.txtecho "IBM,103.1,20160105" >> /tmp/spooldir/dr/.dr.txtmv /tmp/spooldir/dr/.dr.txt /tmp/spooldir/dr/dr.txtIT Certification Guaranteed, The Easy Way!67Requirements:You have been given below directory location (if not available than create it) /tmp/spooldir .You have a finacial subscription for getting stock prices from BloomBerg as well asReuters and using ftp you download every hour new files from their respective ftp site in directories/tmp/spooldir/bb and /tmp/spooldir/dr respectively.As soon as file committed in this directory that needs to be available in hdfs in/tmp/flume/finance location in a single directory.Write a flume configuration file named flume7.conf and use it to load data in hdfs with followingadditional properties .1 . Spool /tmp/spooldir/bb and /tmp/spooldir/dr2 . File prefix in hdfs sholuld be events3 . File suffix should be .log4 . If file is not commited and in use than it should have _ as prefix.5 . Data should be written as text to hdfsAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create directory mkdir /tmp/spooldir/bb mkdir /tmp/spooldir/drStep 2 : Create flume configuration file, with below configuration foragent1.sources = source1 source2agent1 .sinks = sink1agent1.channels = channel1agent1 .sources.source1.channels = channel1agentl .sources.source2.channels = channell agent1 .sinks.sinkl.channel = channell agent1.sources.source1.type = spooldir agent1 .sources.sourcel.spoolDir = /tmp/spooldir/bb agent1.sources.source2.type = spooldiragent1 .sources.source2.spoolDir = /tmp/spooldir/dragent1 .sinks.sink1.type = hdfsagent1 .sinks.sink1.hdfs.path = /tmp/flume/financeagent1-sinks.sink1.hdfs.filePrefix = eventsagent1.sinks.sink1.hdfs.fileSuffix = .logagent1 .sinks.sink1.hdfs.inUsePrefix = _agent1 .sinks.sink1.hdfs.fileType = Data Streamagent1.channels.channel1.type = fileStep 4 : Run below command which will use this configuration file and append data in hdfs.Start flume service:flume-ng agent -conf /home/cloudera/flumeconf -conf-file/home/cloudera/fIumeconf/fIume7.conf --name agent1Step 5 : Open another terminal and create a file in /tmp/spooldir/echo "IBM,100,20160104" > /tmp/spooldir/bb/.bb.txtecho "IBM,103,20160105" > /tmp/spooldir/bb/.bb.txt mv /tmp/spooldir/bb/.bb.txt/tmp/spooldir/bb/bb.txtAfter few minsecho "IBM,100.2,20160104" > /tmp/spooldir/dr/.dr.txtecho "IBM,103.1,20160105" >/tmp/spooldir/dr/.dr.txt mv /tmp/spooldir/dr/.dr.txtIT Certification Guaranteed, The Easy Way!68/tmp/spooldir/dr/dr.txtNO.77 CORRECT TEXTProblem Scenario 30 : You have been given three csv files in hdfs as below.EmployeeName.csv with the field (id, name)EmployeeManager.csv (id, manager Name)EmployeeSalary.csv (id, Salary)Using Spark and its API you have to generate a joined output as below and save as a text tile(Separated by comma) for final distribution and output must be sorted by id.ld,name,salary,managerNameEmployeeManager.csvE01,VishnuE02,SatyamE03,ShivE04,SundarE05,JohnE06,PallaviE07,TanvirE08,ShekharE09,VinodE10,JitendraEmployeeName.csvE01,LokeshE02,BhupeshE03,AmitE04,RatanE05,DineshE06,PavanE07,TejasE08,SheelaE09,KumarE10,VenkatEmployeeSalary.csvE01,50000E02,50000E03,45000E04,45000E05,50000E06,45000E07,50000E08,10000E09,10000E10,10000Answer:See the explanation for Step by Step Solution and configuration.Explanation:IT Certification Guaranteed, The Easy Way!69Solution :Step 1 : Create all three files in hdfs in directory called sparkl (We will do using Hue}.However, you can first create in local filesystem and thenStep 2 : Load EmployeeManager.csv file from hdfs and create PairRDDsval manager = sc.textFile("spark1/EmployeeManager.csv")val managerPairRDD = manager.map(x=> (x.split(",")(0),x.split(",")(1)))Step 3 : Load EmployeeName.csv file from hdfs and create PairRDDsval name = sc.textFile("spark1/EmployeeName.csv")val namePairRDD = name.map(x=> (x.split(",")(0),x.split('\")(1)))Step 4 : Load EmployeeSalary.csv file from hdfs and create PairRDDsval salary = sc.textFile("spark1/EmployeeSalary.csv")val salaryPairRDD = salary.map(x=> (x.split(",")(0),x.split(",")(1)))Step 4 : Join all pairRDDSval joined = namePairRDD.join(salaryPairRDD}.join(managerPairRDD}Step 5 : Now sort the joined results, val joinedData = joined.sortByKey()Step 6 : Now generate comma separated data.val finalData = joinedData.map(v=> (v._1, v._2._1._1, v._2._1._2, v._2._2))Step 7 : Save this output in hdfs as text file.finalData.saveAsTextFile("spark1/result.txt")NO.78 CORRECT TEXTProblem Scenario 75 : You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.orderstable=retail_db.order_itemsjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following activities.1. Copy "retail_db.order_items" table to hdfs in respective directory p90_order_items .2. Do the summation of entire revenue in this table using pyspark.3. Find the maximum and minimum revenue as well.4. Calculate average revenueColumns of ordeMtems table : (order_item_id , order_item_order_id ,order_item_product_id, order_item_quantity,order_item_subtotal,order_item_subtotal,order_item_product_price)Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Import Single table .sqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -password=cloudera -table=order_items --target -dir=p90 ordeMtems --m 1Note : Please check you dont have space between before or after '=' sign. Sqoop uses theMapReduce framework to copy data from RDBMS to hdfsStep 2 : Read the data from one of the partition, created using above command. hadoop fsIT Certification Guaranteed, The Easy Way!70-cat p90_order_items/part-m-00000Step 3 : In pyspark, get the total revenue across all days and orders. entire TableRDD =sc.textFile("p90_order_items")#Cast string to floatextractedRevenueColumn = entireTableRDD.map(lambda line: float(line.split(",")[4]))Step 4 : Verify extracted datafor revenue in extractedRevenueColumn.collect():print revenue#use reduce'function to sum a single column valetotalRevenue = extractedRevenueColumn.reduce(lambda a, b: a + b)Step 5 : Calculate the maximum revenuemaximumRevenue = extractedRevenueColumn.reduce(lambda a, b: (a if a>=b else b))Step 6 : Calculate the minimum revenueminimumRevenue = extractedRevenueColumn.reduce(lambda a, b: (a if a<=b else b))Step 7 : Caclculate average revenuecount=extractedRevenueColumn.count()averageRev=totalRevenue/countNO.79 CORRECT TEXTProblem Scenario 95 : You have to run your Spark application on yarn with each executorMaximum heap size to be 512MB and Number of processor cores to allocate on each executor will be1 and Your main application required three values as input arguments V1V2 V3.Please replace XXX, YYY, ZZZ./bin/spark-submit -class com.hadoopexam.MyTask --master yarn-cluster--num-executors 3--driver-memory 512m XXX YYY lib/hadoopexam.jarZZZAnswer:See the explanation for Step by Step Solution and configuration.Explanation:SolutionXXX: -executor-memory 512m YYY: -executor-cores 1ZZZ : V1 V2 V3Notes : spark-submit on yarn options Option Descriptionarchives Comma-separated list of archives to be extracted into the working directory of eachexecutor. The path must be globally visible inside your cluster; see AdvancedDependency Management.executor-cores Number of processor cores to allocate on each executor. Alternatively, you can usethe spark.executor.cores property, executor-memory Maximum heap size to allocate to eachexecutor. Alternatively, you can use the spark.executor.memory-property.num-executors Total number of YARN containers to allocate for this application.Alternatively, you can use the spark.executor.instances property. queue YARN queue to submit to.For more information, see Assigning Applications and Queries to ResourcePools. Default: default.NO.80 CORRECT TEXTProblem Scenario 1:IT Certification Guaranteed, The Easy Way!71You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.categoriesjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following activities.1 . Connect MySQL DB and check the content of the tables.2 . Copy "retaildb.categories" table to hdfs, without specifying directory name.3 . Copy "retaildb.categories" table to hdfs, in a directory name "categories_target".4 . Copy "retaildb.categories" table to hdfs, in a warehouse directory name"categories_warehouse".Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Connecting to existing MySQL Database mysql --user=retail_dba -- password=clouderaretail_dbStep 2 : Show all the available tables show tables;Step 3 : View/Count data from a table in MySQL select count(1} from categories;Step 4 : Check the currently available data in HDFS directory hdfs dfs -IsStep 5 : Import Single table (Without specifying directory).sqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -password=cloudera -table=categoriesNote : Please check you dont have space between before or after '=' sign. Sqoop uses theMapReduce framework to copy data from RDBMS to hdfsStep 6 : Read the data from one of the partition, created using above command, hdfs dfs -catxategories/part-m-00000Step 7 : Specifying target directory in import command (We are using number of mappers= 1, you can change accordingly) sqoop import -connectjdbc:mysql://quickstart:3306/retail_db -username=retail_dba -password=cloudera~ table=categories -target-dir=categortes_target --m 1Step 8 : Check the content in one of the partition file.hdfs dfs -cat categories_target/part-m-00000Step 9 : Specifying parent directory so that you can copy more than one table in a specified targetdirectory. Command to specify warehouse directory.sqoop import -.-connect jdbc:mysql://quickstart:3306/retail_db --username=retail dba -password=cloudera -table=categories -warehouse-dir=categories_warehouse --m 1NO.81 CORRECT TEXTProblem Scenario 56 : You have been given below code snippet.val a = sc.parallelize(l to 100. 3)operation1Write a correct code snippet for operationl which will produce desired output, shown below.Array [Array [I nt]] = Array(Array(1, 2, 3,4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18,19, 20,21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33),IT Certification Guaranteed, The Easy Way!72Array(34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,5 6, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66),Array(67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88,8 9, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution : a.glom.collectglomAssembles an array that contains all elements of the partition and embeds it in an RDD.Each returned array contains the contents of one panitionNO.82 CORRECT TEXTProblem Scenario 47 : You have been given below code snippet, with intermediate output.val z = sc.parallelize(List(1,2,3,4,5,6), 2)// lets first print out the contents of the RDD with partition labelsdef myfunc(index: Int, iter: lterator[(lnt)]): lterator[String] = {iter.toList.map(x => "[partID:" + index + ", val: " + x + "]").iterator}//In each run , output could be different, while solving problem assume belowm output only.z.mapPartitionsWithlndex(myfunc).collectres28: Array[String] = Array([partlD:0, val: 1], [partlD:0, val: 2], [partlD:0, val: 3], [partlD:1, val: 4],[partlD:1, val: S], [partlD:1, val: 6])Now apply aggregate method on RDD z , with two reduce function , first will select max value in eachpartition and second will add all the maximum values from all partitions.Initialize the aggregate with value 5. hence expected output will be 16.Answer:z.aggregate(5)(math.max(_, J, _ + _)NO.83 CORRECT TEXTProblem Scenario 8 : You have been given following mysql database details as well as other info.Please accomplish following.1. Import joined result of orders and order_items table join on orders.order_id =order_items.order_item_order_id.2 . Also make sure each tables file is partitioned in 2 files e.g. part-00000, part-000023 . Also make sure you use orderid columns for sqoop to use for boundary conditions.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solutions:Step 1 : Clean the hdfs file system, if they exists clean out.hadoop fs -rm -R departmentshadoop fs -rm -R categorieshadoop fs -rm -R productshadoop fs -rm -R ordershadoop fs -rm -R order_itemsIT Certification Guaranteed, The Easy Way!73hadoop fs -rm -R customersStep 2 : Now import the department table as per requirement.sqoop import \--connect jdbc:mysql://quickstart:3306/retail_db \-username=retail_dba \-password=cloudera \-query="select' from orders join order_items on orders.orderid =order_items.order_item_order_id where \SCONDITlONS" \-target-dir /user/cloudera/order_join \-split-by order_id \--num-mappers 2Step 3 : Check imported data.hdfs dfs -Is order_joinhdfs dfs -cat order_join/part-m-00000hdfs dfs -cat order_join/part-m-00001NO.84 CORRECT TEXTProblem Scenario 54 : You have been given below code snippet.val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle")) val b = a.map(x => (x.length,x)) operation1Write a correct code snippet for operationl which will produce desired output, shown below.Array[(lnt, String)] = Array((4,lion), (7,panther), (3,dogcat), (5,tigereagle))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :b.foidByKey("")(_ + J.collectfoldByKey [Pair]Very similar to fold, but performs the folding separately for each key of the RDD. This function is onlyavailable if the RDD consists of two-component tuplesListing Variantsdef foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V}]def foldByKey(zeroValue: V, numPartitions: lnt)(func: (V, V) => V): RDD[(K, V)] deffoldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) => V): RDD[(K, V}]NO.85 CORRECT TEXTProblem Scenario 12 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderadatabase=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following.1. Create a table in retailedb with following definition.CREATE table departments_new (department_id int(11), department_name varchar(45),created_date T1MESTAMP DEFAULT NOW());2 . Now isert records from departments table to departments_newIT Certification Guaranteed, The Easy Way!743 . Now import data from departments_new table to hdfs.4 . Insert following 5 records in departmentsnew table. Insert into departments_new values(110,"Civil" , null); Insert into departments_new values(111, "Mechanical" , null);Insert into departments_new values(112, "Automobile" , null); Insert into departments_newvalues(113, "Pharma" , null);Insert into departments_new values(114, "Social Engineering" , null);5. Now do the incremental import based on created_date column.Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Login to musql dbmysql --user=retail_dba -password=clouderashow databases;use retail db; show tables;Step 2 : Create a table as given in problem statement.CREATE table departments_new (department_id int(11), department_name varchar(45), createddateT1MESTAMP DEFAULT NOW()); show tables;Step 3 : isert records from departments table to departments_new insert into departments_newselect a.", null from departments a;Step 4 : Import data from departments new table to hdfs.sqoop import \-connect jdbc:mysql://quickstart:330G/retail_db \~ username=retail_dba \-password=cloudera \-table departments_new\--target-dir /user/cloudera/departments_new \--split-by departmentsStpe 5 : Check the imported data.hdfs dfs -cat /user/cloudera/departmentsnew/part"Step 6 : Insert following 5 records in departmentsnew table.Insert into departments_new values(110, "Civil" , null);Insert into departments_new values(111, "Mechanical" , null);Insert into departments_new values(112, "Automobile" , null);Insert into departments_new values(113, "Pharma" , null);Insert into departments_new values(114, "Social Engineering" , null);commit;Stpe 7 : Import incremetal data based on created_date column.sqoop import \-connect jdbc:mysql://quickstart:330G/retaiI_db \-username=retail_dba \-password=cloudera \--table departments_new\-target-dir /user/cloudera/departments_new \-append \-check-column created_date \IT Certification Guaranteed, The Easy Way!75-incremental lastmodified \-split-by departments \-last-value "2016-01-30 12:07:37.0"Step 8 : Check the imported value.hdfs dfs -cat /user/cloudera/departmentsnew/part"NO.86 CORRECT TEXTProblem Scenario 57 : You have been given below code snippet.val a = sc.parallelize(1 to 9, 3) operationlWrite a correct code snippet for operationl which will produce desired output, shown below.Array[(String, Seq[lnt])] = Array((even,ArrayBuffer(2, 4, G, 8)), (odd,ArrayBuffer(1, 3, 5, 7,9)))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :a.groupBy(x => {if (x % 2 == 0) "even" else "odd" }).collectNO.87 CORRECT TEXTProblem Scenario 36 : You have been given a file named spark8/data.csv (type,name).data.csv1 ,Lokesh2 ,Bhupesh2 ,Amit2 ,Ratan2 ,Dinesh1 ,Pavan1 ,Tejas2 ,Sheela1 ,Kumar1 ,Venkat1. Load this file from hdfs and save it back as (id, (all names of same type)) in results directory.However, make sure while saving it should beAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Create file in hdfs (We will do using Hue). However, you can first create in local filesystemand then upload it to hdfs.Step 2 : Load data.csv file from hdfs and create PairRDDsval name = sc.textFile("spark8/data.csv")val namePairRDD = name.map(x=> (x.split(",")(0),x.split(",")(1)))Step 3 : Now swap namePairRDD RDD.val swapped = namePairRDD.map(item => item.swap)Step 4 : Now combine the rdd by key.val combinedOutput = namePairRDD.combineByKey(List(_), (x:List[String], y:String) => y ::IT Certification Guaranteed, The Easy Way!76x, (x:List[String], y:List[String]) => x ::: y)Step 5 : Save the output as a Text file and output must be written in a single file.:ombinedOutput.repartition(1).saveAsTextFile("spark8/result.txt")NO.88 CORRECT TEXTProblem Scenario 71 :Write down a Spark script using Python,In which it read a file "Content.txt" (On hdfs) with following content.After that split each row as (key, value), where key is first word in line and entire line as value.Filter out the empty lines.And save this key value in "problem86" as Sequence file(On hdfs)Part 2 : Save as sequence file , where key as null and entire line as value. Read back the storedsequence files.Content.txtHello this is ABCTECH.comThis is XYZTECH.comApache Spark TrainingThis is Spark Learning SessionSpark is faster than MapReduceAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 :# Import SparkContext and SparkConffrom pyspark import SparkContext, SparkConfStep 2:#load data from hdfscontentRDD = sc.textFile(MContent.txt")Step 3:#filter out non-empty linesnonemptyjines = contentRDD.filter(lambda x: len(x) > 0)Step 4:#Split line based on space (Remember : It is mandatory to convert is in tuple} words =nonempty_lines.map(lambda x: tuple(x.split('', 1))) words.saveAsSequenceFile("problem86")Step 5: Check contents in directory problem86 hdfs dfs -cat problem86/part*Step 6 : Create key, value pair (where key is null)nonempty_lines.map(lambda line: (None, Mne}).saveAsSequenceFile("problem86_1")Step 7 : Reading back the sequence file data using spark. seqRDD =sc.sequenceFile("problem86_1")Step 8 : Print the content to validate the same.for line in seqRDD.collect():print(line)NO.89 CORRECT TEXTProblem Scenario 42 : You have been given a file (sparklO/sales.txt), with the content as given inIT Certification Guaranteed, The Easy Way!77below.spark10/sales.txtDepartment,Designation,costToCompany,StateSales,Trainee,12000,UPSales,Lead,32000,APSales,Lead,32000,LASales,Lead,32000,TNSales,Lead,32000,APSales,Lead,32000,TNSales,Lead,32000,LASales,Lead,32000,LAMarketing,Associate,18000,TNMarketing,Associate,18000,TNHR,Manager,58000,TNAnd want to produce the output as a csv with group by Department,Designation,State with additionalcolumns with sum(costToCompany) and TotalEmployeeCounttShould get result likeDept,Desg,state,empCount,totalCostSales,Lead,AP,2,64000Sales.Lead.LA.3.96000Sales,Lead,TN,2,64000Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :step 1 : Create a file first using Hue in hdfs.Step 2 : Load tile as an RDDval rawlines = sc.textFile("spark10/sales.txt")Step 3 : Create a case class, which can represent its column fileds. case classEmployee(dep: String, des: String, cost: Double, state: String)Step 4 : Split the data and create RDD of all Employee objects.val employees = rawlines.map(_.split(",")).map(row=>Employee(row(0), row{1), row{2).toDouble,row{3)))Step 5 : Create a row as we needed. All group by fields as a key and value as a count for eachemployee as well as its cost, val keyVals = employees.map( em => ((em.dep, em.des, em.state), (1 ,em.cost)))Step 6 : Group by all the records using reduceByKey method as we want summation as well. Fornumber of employees and their total cost, val results = keyVals.reduceByKey{(a,b) => (a._1 + b._1, a._2 + b._2)} // (a.count + b.count, a.cost + b.cost)}Step 7 : Save the results in a text file as below.results.repartition(1).saveAsTextFile("spark10/group.txt")NO.90 CORRECT TEXTProblem Scenario 7 : You have been given following mysql database details as well as other info.user=retail_dbapassword=clouderaIT Certification Guaranteed, The Easy Way!78database=retail_dbjdbc URL = jdbc:mysql://quickstart:3306/retail_dbPlease accomplish following.1. Import department tables using your custom boundary query, which import departments between1 to 25.2 . Also make sure each tables file is partitioned in 2 files e.g. part-00000, part-000023 . Also make sure you have imported only two columns from table, which aredepartment_id,department_nameAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solutions :Step 1 : Clean the hdfs tile system, if they exists clean out.hadoop fs -rm -R departmentshadoop fs -rm -R categorieshadoop fs -rm -R productshadoop fs -rm -R ordershadoop fs -rm -R order_itmeshadoop fs -rm -R customersStep 2 : Now import the department table as per requirement.sqoop import \-connect jdbc:mysql://quickstart:3306/retail_db \--username=retail_dba \-password=cloudera \-table departments \-target-dir /user/cloudera/departments \-m2\-boundary-query "select 1, 25 from departments" \-columns department_id,department_nameStep 3 : Check imported data.hdfs dfs -Is departmentshdfs dfs -cat departments/part-m-00000hdfs dfs -cat departments/part-m-00001NO.91 CORRECT TEXTProblem Scenario 64 : You have been given below code snippet.val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3) val b = a.keyBy(_.length) valc = sc.parallelize(Ust("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3) val d =c.keyBy(_.length) operation1Write a correct code snippet for operationl which will produce desired output, shown below.Array[(lnt, (Option[String], String))] = Array((6,(Some(salmon),salmon)),(6,(Some(salmon),rabbit}}, (6,(Some(salmon),turkey)), (6,(Some(salmon),salmon)),(6,(Some(salmon),rabbit)), (6,(Some(salmon),turkey)), (3,(Some(dog),dog)),(3,(Some(dog),cat)), (3,(Some(dog),gnu)), (3,(Some(dog),bee)), (3,(Some(rat),(3,(Some(rat),cat)), (3,(Some(rat),gnu)), (3,(Some(rat),bee)), (4,(None,wo!f)),(4,(None,bear)))IT Certification Guaranteed, The Easy Way!79Answer:See the explanation for Step by Step Solution and configuration.Explanation:solution : b.rightOuterJqin(d).collectrightOuterJoin [Pair] : Performs an right outer join using two key-value RDDs. Please note that thekeys must be generally comparable to make this work correctly.NO.92 CORRECT TEXTProblem Scenario 77 : You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.orderstable=retail_db.order_itemsjdbc URL = jdbc:mysql://quickstart:3306/retail_dbColumns of order table : (orderid , order_date , order_customer_id, order_status)Columns of ordeMtems table : (order_item_id , order_item_order_ld ,order_item_product_id, order_item_quantity,order_item_subtotal,order_item_product_price)Please accomplish following activities.1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directoryp92_orders and p92 order items .2 . Join these data using orderid in Spark and Python3 . Calculate total revenue perday and per order4. Calculate total and average revenue for each date. - combineByKey-aggregateByKeyAnswer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Import Single table .sqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -password=cloudera -table=orders --target-dir=p92_orders -m 1 sqoop import --connectjdbc:mysql://quickstart:3306/retail_db --username=retail_dba - password=cloudera-table=order_items --target-dir=p92_order_items -m1Note : Please check you dont have space between before or after '=' sign. Sqoop uses theMapReduce framework to copy data from RDBMS to hdfsStep 2 : Read the data from one of the partition, created using above command, hadoop fs-cat p92_orders/part-m-00000 hadoop fs -cat p92_order_items/part-m-00000Step 3 : Load these above two directory as RDD using Spark and Python (Open pyspark terminal anddo following). orders = sc.textFile("p92_orders") orderltems = sc.textFile("p92_order_items")Step 4 : Convert RDD into key value as (orderjd as a key and rest of the values as a value)# First value is orderjdordersKeyValue = orders.map(lambda line: (int(line.split(",")[0]), line))# Second value as an OrderjdorderltemsKeyValue = orderltems.map(lambda line: (int(line.split(",")[1]), line))IT Certification Guaranteed, The Easy Way!80Step 5 : Join both the RDD using orderjdjoinedData = orderltemsKeyValue.join(ordersKeyValue)#print the joined datafor line in joinedData.collect():print(line)Format of joinedData as below.[Orderld, 'All columns from orderltemsKeyValue', 'All columns from orders Key Value']Step 6 : Now fetch selected values Orderld, Order date and amount collected on this order.//Retruned row will contain ((order_date,order_id),amout_collected)revenuePerDayPerOrder = joinedData.map(lambda row: ((row[1][1].split(M,M)[1],row[0]},float(row[1][0].split(",")[4])))#print the resultfor line in revenuePerDayPerOrder.collect():print(line)Step 7 : Now calculate total revenue perday and per orderA. Using reduceByKeytotalRevenuePerDayPerOrder = revenuePerDayPerOrder.reduceByKey(lambdarunningSum, value: runningSum + value)for line in totalRevenuePerDayPerOrder.sortByKey().collect(): print(line)#Generate data as (date, amount_collected) (Ignore ordeMd)dateAndRevenueTuple = totalRevenuePerDayPerOrder.map(lambda line: (line[0][0], line[1])) for linein dateAndRevenueTuple.sortByKey().collect(): print(line)Step 8 : Calculate total amount collected for each day. And also calculate number of days.# Generate output as (Date, Total Revenue for date, total_number_of_dates)# Line 1 : it will generate tuple (revenue, 1)# Line 2 : Here, we will do summation for all revenues at the same time another counter to maintainnumber of records.#Line 3 : Final function to merge all the combinertotalRevenueAndTotalCount = dateAndRevenueTuple.combineByKey( \lambda revenue: (revenue, 1), \lambda revenueSumTuple, amount: (revenueSumTuple[0] + amount, revenueSumTuple[1]+ 1), \lambda tuplel, tuple2: (round(tuple1[0] + tuple2[0], 2}, tuple1[1] + tuple2[1]) \ for line intotalRevenueAndTotalCount.collect(): print(line)Step 9 : Now calculate average for each dateaverageRevenuePerDate = totalRevenueAndTotalCount.map(lambda threeElements:(threeElements[0], threeElements[1][0]/threeElements[1][1]}}for line in averageRevenuePerDate.collect(): print(line)Step 10 : Using aggregateByKey#line 1 : (Initialize both the value, revenue and count)#line 2 : runningRevenueSumTuple (Its a tuple for total revenue and total record count for each date)# line 3 : Summing all partitions revenue and counttotalRevenueAndTotalCount = dateAndRevenueTuple.aggregateByKey( \(0,0), \lambda runningRevenueSumTuple, revenue: (runningRevenueSumTuple[0] + revenue,runningRevenueSumTuple[1] + 1), \ lambda tupleOneRevenueAndCount,IT Certification Guaranteed, The Easy Way!81tupleTwoRevenueAndCount:(tupleOneRevenueAndCount[0] + tupleTwoRevenueAndCount[0],tupleOneRevenueAndCount[1] + tupleTwoRevenueAndCount[1]) \)for line in totalRevenueAndTotalCount.collect(): print(line)Step 11 : Calculate the average revenue per dateaverageRevenuePerDate = totalRevenueAndTotalCount.map(lambda threeElements:(threeElements[0], threeElements[1][0]/threeElements[1][1]))for line in averageRevenuePerDate.collect(): print(line)NO.93 CORRECT TEXTProblem Scenario 76 : You have been given MySQL DB with following details.user=retail_dbapassword=clouderadatabase=retail_dbtable=retail_db.orderstable=retail_db.order_itemsjdbc URL = jdbc:mysql://quickstart:3306/retail_dbColumns of order table : (orderid , order_date , ordercustomerid, order_status}.....Please accomplish following activities.1 . Copy "retail_db.orders" table to hdfs in a directory p91_orders.2 . Once data is copied to hdfs, using pyspark calculate the number of order for each status.3 . Use all the following methods to calculate the number of order for each status. (You need to knowall these functions and its behavior for real exam)- countByKey()-groupByKey()- reduceByKey()-aggregateByKey()- combineByKey()Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :Step 1 : Import Single tablesqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail dba -password=cloudera -table=orders --target-dir=p91_ordersNote : Please check you dont have space between before or after '=' sign. Sqoop uses theMapReduce framework to copy data from RDBMS to hdfsStep 2 : Read the data from one of the partition, created using above command, hadoop fs-cat p91_orders/part-m-00000Step 3: countByKey #Number of orders by status allOrders = sc.textFile("p91_orders")#Generate key and value pairs (key is order status and vale as an empty string keyValue =aIIOrders.map(lambda line: (line.split(",")[3], ""))#Using countByKey, aggregate data based on status as a keyoutput=keyValue.countByKey()Jtems()IT Certification Guaranteed, The Easy Way!82for line in output: print(line)Step 4 : groupByKey#Generate key and value pairs (key is order status and vale as an onekeyValue = allOrders.map(lambda line: (line.split)",")[3], 1))#Using countByKey, aggregate data based on status as a key output=keyValue.groupByKey().map(lambda kv: (kv[0], sum(kv[1]}}}tor line in output.collect(): print(line}Step 5 : reduceByKey#Generate key and value pairs (key is order status and vale as an onekeyValue = allOrders.map(lambda line: (line.split(","}[3], 1))#Using countByKey, aggregate data based on status as a key output=keyValue.reduceByKey(lambda a, b: a + b)tor line in output.collect(): print(line}Step 6: aggregateByKey#Generate key and value pairs (key is order status and vale as an one keyValue =allOrders.map(lambda line: (line.split(",")[3], line}} output=keyValue.aggregateByKey(0, lambda a, b:a+1, lambda a, b: a+b} for line in output.collect(): print(line}Step 7 : combineByKey#Generate key and value pairs (key is order status and vale as an onekeyValue = allOrders.map(lambda line: (line.split(",")[3], line))output=keyValue.combineByKey(lambda value: 1, lambda ace, value: acc+1, lambda ace, value:acc+value) tor line in output.collect(): print(line)#Watch Spark Professional Training provided by www.ABCTECH.com to understand more on eachabove functions. (These are very important functions for real exam)NO.94 CORRECT TEXTProblem Scenario 65 : You have been given below code snippet.val a = sc.parallelize(List("dog", "cat", "owl", "gnu", "ant"), 2)val b = sc.parallelize(1 to a.count.tolnt, 2)val c = a.zip(b)operation1Write a correct code snippet for operationl which will produce desired output, shown below.Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,1), (cat,2>, (ant,5))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution : c.sortByKey(false).collectsortByKey [Ordered] : This function sorts the input RDD's data and stores it in a new RDD."The output RDD is a shuffled RDD because it stores data that is output by a reducer which has beenshuffled. The implementation of this function is actually very clever.First, it uses a range partitioner to partition the data in ranges within the shuffled RDD.Then it sorts these ranges individually with mapPartitions using standard sort mechanisms.NO.95 CORRECT TEXTProblem Scenario 51 : You have been given below code snippet.val a = sc.parallelize(List(1, 2,1, 3), 1)IT Certification Guaranteed, The Easy Way!83val b = a.map((_, "b"))val c = a.map((_, "c"))Operation_xyzWrite a correct code snippet for Operationxyz which will produce below output.Output:Array[(lnt, (lterable[String], lterable[String]))] = Array((2,(ArrayBuffer(b),ArrayBuffer(c))),(3,(ArrayBuffer(b),ArrayBuffer(c))),(1,(ArrayBuffer(b, b),ArrayBuffer(c, c))))Answer:See the explanation for Step by Step Solution and configuration.Explanation:Solution :b.cogroup(c).collectcogroup [Pair], groupWith [Pair]A very powerful set of functions that allow grouping up to 3 key-value RDDs together using their keys.Another exampleval x = sc.parallelize(List((1, "apple"), (2, "banana"), (3, "orange"), (4, "kiwi")), 2) val y =sc.parallelize(List((5, "computer"), (1, "laptop"), (1, "desktop"), (4, "iPad")), 2) x.cogroup(y).collectArray[(lnt, (lterable[String], lterable[String]))] = Array((4,(ArrayBuffer(kiwi),ArrayBuffer(iPad))),(2,(ArrayBuffer(banana),ArrayBuffer())),(3,(ArrayBuffer(orange),ArrayBuffer())),(1 ,(ArrayBuffer(apple),ArrayBuffer(laptop, desktop))),(5,{ArrayBuffer(),ArrayBuffer(computer))))NO.96 CORRECT TEXTProblem Scenario 93 : You have to run your Spark application with locally 8 thread or locally on 8cores. Replace XXX with correct values.spark-submit --class com.hadoopexam.MyTask XXX \ -deploy-mode clusterSSPARK_HOME/lib/hadoopexam.jar 10Answer:See the explanation for Step by Step Solution and configuration.Explanation:SolutionXXX: -master local[8]Notes : The master URL passed to Spark can be in one of the following formats:Master URL Meaninglocal Run Spark locally with one worker thread (i.e. no parallelism at all}.local[K] Run Spark locally with K worker threads (ideally, set this to the number of cores on yourmachine).local[*] Run Spark locally with as many worker threads as logical cores on your machine.spark://HOST:PORT Connect to the given Spark standalone cluster master. The port must bewhichever one your master is configured to use, which is 7077 by default.IT Certification Guaranteed, The Easy Way!84mesos://HOST:PORT Connect to the given Mesos cluster. The port must be whichever one your isconfigured to use, which is 5050 by default. Or, for a Mesos cluster usingZooKeeper, use mesos://zk://.... To submit with --deploy-mode cluster, the HOST:PORT should beconfigured to connect to the MesosClusterDispatcher.yarn Connect to a YARN cluster in client or cluster mode depending on the value of - deploy-mode.The cluster location will be found based on the HADOOP CONF DIR orYARN CONF DIR variable.IT Certification Guaranteed, The Easy Way!85